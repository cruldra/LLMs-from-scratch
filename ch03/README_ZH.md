# 第3章：编码注意力机制

&nbsp;
## 主要章节代码

- [01_main-chapter-code](01_main-chapter-code) 包含主要章节代码。

&nbsp;
## 额外材料

- [02_bonus_efficient-multihead-attention](02_bonus_efficient-multihead-attention) 实现并比较了多头注意力的不同实现变体
- [03_understanding-buffers](03_understanding-buffers) 解释了PyTorch缓冲区背后的思想，这些缓冲区用于在第3章中实现因果注意力机制



在下面的视频中，我提供了一个代码跟随会话，涵盖了一些章节内容作为补充材料。

<br>
<br>

[![链接到视频](https://img.youtube.com/vi/-Ll8DtpNtvk/0.jpg)](https://www.youtube.com/watch?v=-Ll8DtpNtvk)
