{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45398736-7e89-4263-89c8-92153baff553",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "《<a href=\"http://mng.bz/orYv\">从零开始构建大型语言模型</a>》一书的补充代码，作者：<a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>代码仓库：<a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd524e-864c-4012-b0a2-ccfc56e80024",
   "metadata": {
    "id": "66dd524e-864c-4012-b0a2-ccfc56e80024"
   },
   "source": [
    "# 第5章：在无标签数据上进行预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "92b989e9-da36-4159-b212-799184764dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.5\n",
      "numpy version: 2.0.2\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.7.1\n",
      "tensorflow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3bdf9e-2ff0-4a57-abab-ede2d955a237",
   "metadata": {},
   "source": [
    "- 在本章中，我们实现训练循环和基本模型评估代码来预训练LLM\n",
    "- 在本章末尾，我们还将OpenAI的公开可用预训练权重加载到我们的模型中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd27fcc-2886-47cb-b544-046c2c31f02a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/chapter-overview.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d214765-7a73-42d5-95e9-302154b29db9",
   "metadata": {},
   "source": [
    "- 本章涵盖的主题如下所示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67711d4-8391-4fee-aeef-07ea53dd5841",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model--0.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d824183-145c-4865-89e1-1f0d0a338f19",
   "metadata": {
    "id": "0d824183-145c-4865-89e1-1f0d0a338f19"
   },
   "source": [
    "## 5.1 评估生成式文本模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3350f8c-5181-4f9b-a789-4523105e98f2",
   "metadata": {},
   "source": [
    "- 我们从简要回顾使用前一章代码初始化GPT模型开始本节\n",
    "- 然后，我们讨论LLM的基本评估指标\n",
    "- 最后，在本节中，我们将这些评估指标应用于训练和验证数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd",
   "metadata": {
    "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd"
   },
   "source": [
    "### 5.1.1 使用GPT生成文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3415fd-9f4a-4548-908e-9dfa56edc9bc",
   "metadata": {},
   "source": [
    "- 我们使用前一章的代码初始化一个GPT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "86000d74-624a-48f0-86da-f41926cb9e04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86000d74-624a-48f0-86da-f41926cb9e04",
    "outputId": "ad482cfd-5a62-4f0d-e1e0-008d6457f512"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "# 如果本地没有 `previous_chapters.py` 文件，\n",
    "# 你可以从 `llms-from-scratch` PyPI 包中导入。\n",
    "# 详情请见：https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# 例如：\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # 词汇表大小\n",
    "    \"context_length\": 256, # 缩短的上下文长度（原始：1024）\n",
    "    \"emb_dim\": 768,        # 嵌入维度\n",
    "    \"n_heads\": 12,         # 注意力头数\n",
    "    \"n_layers\": 12,        # 层数\n",
    "    \"drop_rate\": 0.1,      # Dropout率\n",
    "    \"qkv_bias\": False      # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # 在推理期间禁用dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6cf0f-7458-48a2-97fd-aa5068d65e8c",
   "metadata": {},
   "source": [
    "- 我们在上面使用了0.1的dropout，但现在训练LLM时不使用dropout是相对常见的\n",
    "- 现代LLM也不在查询、键和值矩阵的`nn.Linear`层中使用偏置向量（与早期的GPT模型不同），这通过设置`\"qkv_bias\": False`来实现\n",
    "- 我们将上下文长度（`context_length`）减少到仅256个token，以减少训练模型的计算资源需求，而原始的1.24亿参数GPT-2模型使用1024个token\n",
    "  - 这样做是为了让更多读者能够在笔记本电脑上跟随和执行代码示例\n",
    "  - 但是，请随意将`context_length`增加到1024个token（这不需要任何代码更改）\n",
    "  - 我们稍后也会从预训练权重加载一个具有1024`context_length`的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f80895-be35-4bb5-81cb-f357ef7367fe",
   "metadata": {},
   "source": [
    "- 接下来，我们使用前一章的`generate_text_simple`函数来生成文本\n",
    "- 此外，我们定义两个便利函数，`text_to_token_ids`和`token_ids_to_text`，用于在token和文本表示之间转换，我们在本章中会使用这些函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741881f3-cee0-49ad-b11d-b9df3b3ac234",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-process.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5e062b82-3540-48ce-8eb4-009686d0d16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "# 或者：\n",
    "# from llms_from_scratch.ch04 import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # 添加批次维度\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # 移除批次维度\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d3249b-b2a0-44c4-b589-ae4b403b8305",
   "metadata": {},
   "source": [
    "- 如上所示，模型没有产生好的文本，因为它还没有被训练\n",
    "- 我们如何以数字形式测量或捕获什么是\"好文本\"，以便在训练期间跟踪它？\n",
    "- 下一小节介绍了计算生成输出损失指标的指标，我们可以用它来测量训练进度\n",
    "- 关于微调LLM的下一章也将介绍测量模型质量的其他方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f9e1a-7bf7-40d8-b1fa-eacabdee8d8e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98",
   "metadata": {
    "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98"
   },
   "source": [
    "### 5.1.2 计算文本生成损失：交叉熵和困惑度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ba8aa-fb03-4d25-957f-fe8778762440",
   "metadata": {},
   "source": [
    "- 假设我们有一个包含2个训练示例（行）的token ID的`inputs`张量\n",
    "- 对应于`inputs`，`targets`包含我们希望模型生成的期望token ID\n",
    "- 注意`targets`是`inputs`向右移动1个位置，正如我们在第2章实现数据加载器时所解释的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f",
    "outputId": "8d6fa0ff-7b37-4634-c3f0-2c050cbe81f0"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc0645-ac2c-4973-9b40-6da40515bede",
   "metadata": {},
   "source": [
    "- 将`inputs`输入到模型中，我们获得由3个token组成的2个输入示例的logits向量\n",
    "- 每个token都是一个50,257维的向量，对应于词汇表的大小\n",
    "- 应用softmax函数，我们可以将logits张量转换为包含概率分数的相同维度张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e7b6ec51-6f8c-49bd-a349-95ba38b46fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # 词汇表中每个token的概率\n",
    "print(probas.shape) # 形状：(batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c36a382-b5e2-4de6-9e65-0b69b685013b",
   "metadata": {},
   "source": [
    "- 下图使用一个非常小的词汇表进行说明，概述了我们如何将概率分数转换回文本，这是我们在前一章末尾讨论的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d86a9-0013-476c-bb6b-274fd5f20b29",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-to-text.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8480efd-d419-4954-9ecc-2876055334bd",
   "metadata": {},
   "source": [
    "- 如前一章所讨论的，我们可以应用`argmax`函数将概率分数转换为预测的token ID\n",
    "- 上面的softmax函数为每个token产生了一个50,257维的向量；`argmax`函数返回该向量中最高概率分数的位置，这是给定token的预测token ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b84c9f-dd08-482e-b903-a86fe44e1144",
   "metadata": {},
   "source": [
    "- 由于我们有2个输入批次，每个批次有3个token，我们得到2×3的预测token ID："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
    "outputId": "ed17da47-c3e7-4775-fd00-4ec5bcda3db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4072c-21ed-4df7-8721-dd2535362573",
   "metadata": {},
   "source": [
    "- 如果我们解码这些token，我们发现它们与我们希望模型预测的token（即目标token）相当不同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c990ead6-53cd-49a7-a6d1-14d8c1518249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53eb8a7-070e-46d6-930c-314ba55a6ff2",
   "metadata": {},
   "source": [
    "- 这是因为模型还没有被训练\n",
    "- 为了训练模型，我们需要知道它与正确预测（目标）的距离有多远"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90592f-0d5d-4ec8-9ff5-e7675beab10e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-index.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7251bf5-a079-4782-901d-68c9225d3157",
   "metadata": {},
   "source": [
    "- 对应于目标索引的token概率如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
    "outputId": "41c946a2-c458-433e-a53d-5e7e89d9dddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e89a19-73c2-4e49-93b4-861f699f1cbf",
   "metadata": {},
   "source": [
    "- 我们希望最大化所有这些值，使它们接近概率1\n",
    "- 在数学优化中，最大化概率分数的对数比最大化概率分数本身更容易；这超出了本书的范围，但我在这里录制了一个包含更多细节的讲座：[L8.2 逻辑回归损失函数](https://www.youtube.com/watch?v=GxJe0DZvydM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
    "outputId": "1bf18e79-1246-4eab-efd8-12b328c78678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# 计算所有token概率的对数\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4261441-a511-4633-9c4c-67998af31b84",
   "metadata": {},
   "source": [
    "- 接下来，我们计算平均对数概率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9b003797-161b-4d98-81dc-e68320e09fec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b003797-161b-4d98-81dc-e68320e09fec",
    "outputId": "a447fe9c-7e27-40ed-f1fb-51210e3f7cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# 计算每个token的平均概率\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d51994-ad17-4ba3-a6ec-f588b4b13585",
   "metadata": {},
   "source": [
    "- 目标是通过优化模型权重使这个平均对数概率尽可能大\n",
    "- 由于对数的性质，最大可能值是0，我们目前距离0还很远"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de388a1-8a0a-4c94-8894-9041dc6ad514",
   "metadata": {},
   "source": [
    "- 在深度学习中，不是最大化平均对数概率，而是标准惯例是最小化*负*平均对数概率值；在我们的情况下，不是最大化-10.7722使其接近0，在深度学习中，我们会最小化10.7722使其接近0\n",
    "- -10.7722的负值，即10.7722，在深度学习中也称为交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "176ddf35-1c5f-4d7c-bf17-70f3e7069bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eeb868-abd8-4028-82db-107546bf7c2c",
   "metadata": {},
   "source": [
    "- PyTorch已经实现了一个`cross_entropy`函数来执行前面的步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd24b7f-b760-47ad-bc84-86d13794aa54",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/cross-entropy.webp?123\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aaf9dd-3ee6-42bf-a63f-6e93dbfb989d",
   "metadata": {},
   "source": [
    "- 在应用`cross_entropy`函数之前，让我们检查logits和targets的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
    "outputId": "43fd802a-8136-4b35-df0d-f61a5d4cb561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits的形状为(batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets的形状为(batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d65f0-6566-4865-93e4-0c0bcb10cd06",
   "metadata": {},
   "source": [
    "- 对于PyTorch中的`cross_entropy`函数，我们希望通过在批次维度上组合来展平这些张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
    "outputId": "0b2b778b-02fb-43b2-c879-adc59055a7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921a57f-3a79-473e-a863-6d63b495010f",
   "metadata": {},
   "source": [
    "- 注意targets是token ID，它们也代表我们想要最大化的logits张量中的索引位置\n",
    "- PyTorch中的`cross_entropy`函数将自动处理在要最大化的logits中那些token索引上应用softmax和对数概率计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
    "outputId": "c0be634a-2c65-4ff7-a73f-1bfc2e406ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15ce17-fd7b-4d8e-99da-b237523a7a80",
   "metadata": {},
   "source": [
    "- 与交叉熵损失相关的一个概念是LLM的困惑度\n",
    "- 困惑度简单地是交叉熵损失的指数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "168952a1-b964-4aa7-8e49-966fa26add54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "168952a1-b964-4aa7-8e49-966fa26add54",
    "outputId": "a0a692c1-6412-4068-8aa5-8858548141eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae26dd-d77e-41fd-b924-6bd103dd4ee7",
   "metadata": {},
   "source": [
    "- 困惑度通常被认为更具可解释性，因为它可以理解为模型在每一步不确定的有效词汇表大小（在上面的例子中，那将是48,725个单词或token）\n",
    "- 换句话说，困惑度提供了一个衡量模型预测的概率分布与数据集中单词的实际分布匹配程度的指标\n",
    "- 与损失类似，较低的困惑度表明模型预测更接近实际分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487",
   "metadata": {
    "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487"
   },
   "source": [
    "### 5.1.3 计算训练和验证集损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530da89e-2448-436c-8f1b-28e8a31ef85c",
   "metadata": {},
   "source": [
    "- 我们使用一个相对较小的数据集来训练LLM（实际上只有一个短篇小说）\n",
    "- 原因是：\n",
    "  - 你可以在没有合适GPU的笔记本电脑上在几分钟内运行代码示例\n",
    "  - 训练完成相对较快（几分钟而不是几周），这对教育目的很好\n",
    "  - 我们使用公共领域的文本，可以包含在这个GitHub仓库中而不违反任何使用权利或使仓库大小膨胀\n",
    "\n",
    "\n",
    "- 例如，Llama 2 7B需要在A100 GPU上训练184,320个GPU小时，处理2万亿个token\n",
    "  - 在撰写本文时，AWS上8xA100云服务器的每小时成本约为\\\\$30\n",
    "  - 因此，通过粗略计算，训练这个LLM的成本为184,320 / 8 * \\\\$30 = \\\\$690,000\n",
    " \n",
    "- 下面，我们使用第2章中使用的相同数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "654fde37-b2a9-4a20-a8d3-0206c056e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379330f1-80f4-4e34-8724-41d892b04cee",
   "metadata": {},
   "source": [
    "- 通过打印前99个和后99个字符来快速检查文本是否正确加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6kgJbe4ehI4q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6kgJbe4ehI4q",
    "outputId": "9ff31e88-ee37-47e9-ee64-da6eb552f46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# 前99个字符\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "j2XPde_ThM_e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j2XPde_ThM_e",
    "outputId": "a900c1b9-9a87-4078-968b-a5721deda5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "# 后99个字符\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
    "outputId": "c2a25334-21ca-486e-8226-0296e5fc6486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8830cb9-90f6-4e7c-8620-beeabc2d39f7",
   "metadata": {},
   "source": [
    "- 有5,145个token，这个文本对于训练LLM来说非常短，但同样，这是为了教育目的（我们稍后也会加载预训练权重）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcad87-a0e8-4b9d-ac43-4e927ccbb50f",
   "metadata": {},
   "source": [
    "- 接下来，我们将数据集分为训练集和验证集，并使用第2章的数据加载器来为LLM训练准备批次\n",
    "- 为了可视化目的，下图假设`max_length=6`，但对于训练加载器，我们将`max_length`设置为LLM支持的上下文长度\n",
    "- 下图为了简单起见只显示输入token\n",
    "    - 由于我们训练LLM预测文本中的下一个单词，目标看起来与这些输入相同，除了目标向右移动一个位置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdaa07-ba96-4ac1-9d71-b3cc153910d9",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/batching.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0959c855-f860-4358-8b98-bc654f047578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "# 或者：\n",
    "# from llms_from_scratch.ch02 import create_dataloader_v1\n",
    "\n",
    "# 训练/验证比例\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f37b3eb0-854e-4895-9898-fa7d1e67566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 健全性检查\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"训练加载器的token不足。\"\n",
    "          \"尝试降低`GPT_CONFIG_124M['context_length']`或\"\n",
    "          \"增加`training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"验证加载器的token不足。\"\n",
    "          \"尝试降低`GPT_CONFIG_124M['context_length']`或\"\n",
    "          \"减少`training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac3296-a4d1-4303-9ac5-376518960c33",
   "metadata": {},
   "source": [
    "- 我们使用相对较小的批次大小来减少计算资源需求，也因为数据集本身就很小\n",
    "- 例如，Llama 2 7B使用1024的批次大小进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0514d-b990-4dc0-9afb-7721993284a0",
   "metadata": {},
   "source": [
    "- 可选检查数据是否正确加载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ca0116d0-d229-472c-9fbf-ebc229331c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b9b1a4-863d-456f-a8dd-c07fb5c024ed",
   "metadata": {},
   "source": [
    "- 另一个可选检查，确认token大小在预期范围内："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eb860488-5453-41d7-9870-23b723f742a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb860488-5453-41d7-9870-23b723f742a0",
    "outputId": "96b9451a-9557-4126-d1c8-51610a1995ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3085e8-665e-48eb-bb41-cdde61537e06",
   "metadata": {},
   "source": [
    "- 接下来，我们实现一个实用函数来计算给定批次的交叉熵损失\n",
    "- 此外，我们实现第二个实用函数来计算数据加载器中用户指定数量批次的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc",
   "metadata": {
    "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc"
   },
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # 如果num_batches超过数据加载器中的批次数量，\n",
    "        # 则减少批次数量以匹配数据加载器中的总批次数\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0691332-84d0-48b3-b462-a885ddeb4fca",
   "metadata": {},
   "source": [
    "- 如果你有一台支持CUDA的GPU机器，LLM将在GPU上训练，无需对代码进行任何更改\n",
    "- 通过`device`设置，我们确保数据加载到与LLM模型相同的设备上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "56f5b0c9-1065-4d67-98b9-010e42fc1e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98758347829183\n",
      "Validation loss: 10.981106758117676\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 注意：\n",
    "# 取消注释以下行将允许代码在Apple Silicon芯片上运行（如果适用），\n",
    "# 这比在Apple CPU上快约2倍（在M3 MacBook Air上测量）。\n",
    "# 但是，结果损失值可能略有不同。\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # 对于nn.Module类，不需要赋值model = model.to(device)\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # 由于数据加载器中的洗牌，为了可重现性\n",
    "\n",
    "with torch.no_grad(): # 禁用梯度跟踪以提高效率，因为我们还没有训练\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43875e95-190f-4b17-8f9a-35034ba649ec",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-1.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9339f8d-00cb-4206-af67-58c32bd72055",
   "metadata": {
    "id": "b9339f8d-00cb-4206-af67-58c32bd72055"
   },
   "source": [
    "## 5.2 训练LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a4cf4-e98f-46d9-bdec-60e7ccb8d6bd",
   "metadata": {},
   "source": [
    "- 在本节中，我们最终实现了训练LLM的代码\n",
    "- 我们专注于一个简单的训练函数（如果你有兴趣用更高级的技术来增强这个训练函数，如学习率预热、余弦退火和梯度裁剪，请参考[附录D](../../appendix-D/01_main-chapter-code)）\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/train-steps.webp\" width=300px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "Mtp4gY0ZO-qq",
   "metadata": {
    "id": "Mtp4gY0ZO-qq"
   },
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # 初始化列表来跟踪损失和已见token\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # 主训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 设置模型为训练模式\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # 重置前一批次迭代的损失梯度\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # 计算损失梯度\n",
    "            optimizer.step() # 使用损失梯度更新模型权重\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # 可选的评估步骤\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # 每个epoch后打印一个样本文本\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # 紧凑打印格式\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301b333-b9d4-4eeb-a212-3a9874e3ac47",
   "metadata": {},
   "source": [
    "- 现在，让我们使用上面定义的训练函数来训练LLM："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3422000b-7aa2-485b-92df-99372cd22311",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3422000b-7aa2-485b-92df-99372cd22311",
    "outputId": "0e046603-908d-4093-8ae5-ef2f632639fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933\n",
      "Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048\n",
      "Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Ep 3 (Step 000020): Train loss 5.726, Val loss 6.600\n",
      "Ep 3 (Step 000025): Train loss 5.201, Val loss 6.348\n",
      "Every effort moves you, and I had been.                                            \n",
      "Ep 4 (Step 000030): Train loss 4.417, Val loss 6.278\n",
      "Ep 4 (Step 000035): Train loss 4.069, Val loss 6.226\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Ep 5 (Step 000040): Train loss 3.732, Val loss 6.160\n",
      "Every effort moves you know it was not that the picture--I had the fact by the last I had been--his, and in the            \"Oh, and he said, and down the room, and in\n",
      "Ep 6 (Step 000045): Train loss 2.850, Val loss 6.179\n",
      "Ep 6 (Step 000050): Train loss 2.427, Val loss 6.141\n",
      "Every effort moves you know,\" was one of the picture. The--I had a little of a little: \"Yes, and in fact, and in the picture was, and I had been at my elbow and as his pictures, and down the room, I had\n",
      "Ep 7 (Step 000055): Train loss 2.104, Val loss 6.134\n",
      "Ep 7 (Step 000060): Train loss 1.882, Val loss 6.233\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his\n",
      "Ep 8 (Step 000065): Train loss 1.320, Val loss 6.238\n",
      "Ep 8 (Step 000070): Train loss 0.985, Val loss 6.242\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000075): Train loss 0.717, Val loss 6.293\n",
      "Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n"
     ]
    }
   ],
   "source": [
    "# 注意：\n",
    "# 取消注释以下代码来计算执行时间\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 注意：\n",
    "# 取消注释以下代码来显示执行时间\n",
    "# end_time = time.time()\n",
    "# execution_time_minutes = (end_time - start_time) / 60\n",
    "# print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b86f0-b07d-40d7-b9d3-a9218917f204",
   "metadata": {},
   "source": [
    "- 注意你在你的计算机上可能会得到略微不同的损失值，如果它们大致相似（训练损失低于1，验证损失低于7），这不是令人担心的原因\n",
    "- 小的差异通常可能是由于不同的GPU硬件和CUDA版本或较新PyTorch版本中的小变化\n",
    "- 即使你在CPU上运行示例，你也可能观察到轻微的差异；差异的一个可能原因是`nn.Dropout`在不同操作系统上的不同行为，这取决于PyTorch的编译方式，如[PyTorch问题跟踪器上的讨论](https://github.com/pytorch/pytorch/issues/121595)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAATn1JREFUeJzt3QV81PX/B/DXuliwYEFt1OgOaRUkpVQwEAkFpRUDUVFQBEFEBBHrD/wMREVKGumu0Z0jx2Cs2NhY3P/x/ty+t9sYsMG2i72ej8eXq+/dffbl7t7fT75tdDqdDkRERGSWbE1dACIiIro3BmoiIiIzxkBNRERkxhioiYiIzBgDNRERkRljoCYiIjJjDNRERERmjIGaiIjIjDFQExERmTEGaiIrcP78edjY2GD//v2mLgoR5TMGaiIzIYH2ftuYMWNMXUQiMgF7U7wpEd3t6tWrhut//vknPv74Y5w4ccJwX7FixUxUMiIyJdaoicxEQECAYfP09FS1aO12iRIlMGXKFJQqVQpOTk6oXbs2Vq5cec/XSktLQ79+/VC5cmVcuHBB3bd48WLUrVsXzs7OKFeuHMaOHYvU1FTDc+T9fv75Z3Tr1g2urq6oWLEilixZYng8OjoaPXv2hJ+fH1xcXNTjs2fPvmcZ5s+fjxo1aqh9fXx80Lp1ayQkJBgel/eqUqWKKo+U87vvvsvy/IsXL6JHjx7w8vKCt7c3unTpopr4NX369EHXrl0xefJkBAYGqvcYPHgwUlJSHuLoE5kxyZ5FROZl9uzZOk9PT8PtKVOm6Dw8PHR//PGH7vjx47r33ntP5+DgoDt58qR6/Ny5c5IFT7dv3z5dUlKSrlu3bro6deroIiMj1eObNm1Sz58zZ47uzJkzutWrV+uCg4N1Y8aMMbyHPL9UqVK6uXPn6k6dOqUbNmyYrlixYrqoqCj1+ODBg3W1a9fW7d69W73fmjVrdEuWLMmx/FeuXNHZ29urcsu+Bw8e1M2YMUMXHx+vHv/tt990gYGBun/++Ud39uxZdent7a3KJ+7cuaOrUqWKrl+/fuq5R48e1b300ku60NBQXXJystqnd+/e6m964403dMeOHdP9+++/OldXV92PP/5YYP8vRKbAQE1kAYE6KChI9/nnn2fZp0GDBrpBgwZlCdSbN2/WtWrVStesWTNdTEyMYV+5b/z48Vme/+uvv6pgqZHnf/TRR4bbt27dUvetWLFC3e7UqZOub9++uSr/3r171XPPnz+f4+Ply5dXJwTGPvvsM13jxo0NZZOgnJ6ebnhcArSLi4tu1apVhkBdtmxZXWpqqmGf7t27655//vlclZHIUrCPmsjMxcXF4cqVK2jatGmW++X2gQMHstz34osvqubxdevWqSZnjey3detWfP7551max5OSkpCYmKiaukXNmjUNj7u5ucHDwwORkZHq9sCBA/Hss88iLCwMbdq0Uc3OTZo0ybHMtWrVQqtWrVTTd9u2bdX+zz33HIoXL66av8+cOYNXX30V/fv3NzxHmuGlyV8r7+nTp+Hu7p7ldaW88lxNtWrVYGdnZ7gtTeCHDh3K9bElsgQM1ERWpEOHDvjtt9+wfft2PPnkk4b7b926pfqkn3nmmbueI33EGgcHhyyPSb91enq6ut6+fXuEh4dj+fLlWLNmjQrE0icsfcTZSfCUfbZt24bVq1dj+vTp+PDDD7Fz507DScFPP/2ERo0a3fU8rbz16tXD77//ftdrSx95bspLZC0YqInMnNRqg4KCVI24ZcuWhvvldsOGDbPsK7Xe6tWro3Pnzli2bJlhfxlEJiPIK1So8EhlkSDZu3dvtTVv3hzvvvtujoFaC5pS65dNRrCXLVsWCxcuxIgRI9Tfc/bsWTU4LSdSXhn5LoPo5O8nKsoYqIksgATETz75BOXLl1cjvmW0tSxuklONc+jQoapZ++mnn8aKFSvQrFkzFSjldpkyZVQTtK2trWpePnz4MMaNG5erMshrSC1XmpuTk5OxdOlSNWo7J1JzXrt2rWrylmArt69fv27YX2r3w4YNU03d7dq1U6+3Z88eNbJcArkE8C+//FKN9P70009Vc77U5hcsWID33ntP3SYqKhioiSyABLXY2Fi8/fbbqs+4atWqauqUTJHKyZtvvqmagKUpXKZxST+xBFYJehMnTlRNxjIl6rXXXst1GRwdHTFq1Cg1RUr6v6VGPW/evBz3lVrwpk2bMHXqVNXHLrXpr776SjWfC3lfaQKXYCwnIdIfLv3ZUm4hj8nzR44cqZrr4+PjUbJkSdXczho2FTU2MqLM1IUgIiKinHHBEyIiIjPGQE1ERGTGGKiJiIjMGAM1ERGRGWOgJiIiMmMM1ERERGaMgfoeZsyYgeDgYLW8oixzuGvXLlMXySzI3NZOnTqplaVk5alFixZleVxm+8nCGLLmssy1ldSGp06dyrLPzZs31YIWMh9WUhjKms+yZKSxgwcPqnm6cvxLly6NSZMm3VWWv//+W80Fln1kDq4sbWnJJkyYgAYNGqj1rWWREFlL2zgftbbWtSzbKSkdJT+1rL197dq1LPtIWsuOHTuqucjyOjJP2TidpdiwYYNa/UtSZspqZXPmzCkS34GZM2eq9czlsydb48aN1aIwGh7f/PXFF1+o3wltfrzgMX4Ips4KYo7mzZunc3R01M2aNUt35MgRXf/+/XVeXl66a9eu6Yq65cuX6z788EPdggULVHakhQsXZnn8iy++UFmfFi1apDtw4ICuc+fOupCQEN3t27cN+7Rr105Xq1Yt3Y4dO1S2pwoVKuhefPFFw+OxsbE6f39/Xc+ePXWHDx9WqR0la9IPP/xg2Gfr1q06Ozs73aRJk1QKRMn6JGkfDx06pLNUbdu2VVmz5G/ev3+/rkOHDroyZcqoLFYaSelYunRp3dq1a3V79uzRPfbYY7omTZoYHpdMUtWrV9e1bt1apbyU/y9fX1/dqFGjDPtIWklJBzlixAh17KZPn66O5cqVK63+OyBpOZctW6bSg544cUL3wQcfqM+NHHPB45t/du3apVKp1qxZUzd8+HDD/TzGecdAnYOGDRuq3LuatLQ0lWZwwoQJJi2XuckeqCUlYUBAgO7LL7803CepFp2cnFSwFfKlkudJTmONpFG0sbHRXb58Wd3+7rvvdMWLFzfkHRYjR45UaQ81PXr00HXs2DFLeRo1aqR7/fXXddZCcknLsdq4caPhWEpQ+fvvvw37SB5m2Wf79u3qtvyo2dra6iIiIgz7zJw5U+Vt1o6n5LKuVq1alveS1JByolAUvwPyWfv55595fPOR5B2vWLGiylnesmVLQ6DmMX44bPrO5s6dO9i7d69qstXIushyWzIS0b2dO3cOERERWY6drOUsTU7asZNLae6uX7++YR/ZX46xrAet7dOiRQu1ZKVGlsCUZmBZC1rbx/h9tH2s6f9IlgwV3t7e6lI+lykpKVn+bmn6l/W7jY+vdAP4+/tnOS6yjOeRI0dydeyKyndA1kOXJVAl7aY0gfP45h9p2pam6+zHgcf44XCt72xu3LihvsDGHxIht48fP26yclkCCdIip2OnPSaX0udkzN7eXgUj431CQkLueg3tMclpLJf3ex9LJ+t0S7+eZJ6SbFhC/jY5eZETnfsd35yOi/bY/faRH8Lbt2+rkyFr/g5IvmoJzNJXKn2kktFL1k6XJCc8vo9OTn4kZ/nu3bvveoyf4YfDQE1kpjUSyWy1ZcsWUxfF6oSGhqqgLC0W8+fPVyk7N27caOpiWYWLFy9i+PDhKhe5cZ5zejRs+s7G19dXJa/PPgpRbgcEBJisXJZAOz73O3ZyKdmfjMloThkJbrxPTq9h/B732sca/o+GDBmiMl2tX78+SzpH+dukSS8mJua+x/dhj52MgpaR+tb+HZAanYwSlpSdMtK+Vq1a+Oabb3h884E0N8v3W0ZjS0uZbHISNG3aNHVdarQ8xnnHQJ3Dl1i+wJJL17gZUm5LcxndmzRXy5fA+NhJU5T0PWvHTi7lSypfaM26devUMZa+bG0fmQYmfVkaOUOXmpA0e2v7GL+Pto8l/x/J+DwJ0tIUK8cke/O/fC4lPaXx3y399jKVxfj4StOu8cmQHBf5AZPm3dwcu6L2HZC/TfJh8/g+OklDKsdHWiy0TcajyHRM7TqP8UN4yEFoVk2G9ctI5Tlz5qhRygMGDFDD+o1HIRZVMppTpkzIJh+fKVOmqOvh4eGG6VlyrBYvXqw7ePCgrkuXLjlOz6pTp45u586dui1btqjRocbTs2RkqEzP6tWrl5o2I/8fMhUj+/Qse3t73eTJk9Wo0U8++cTip2cNHDhQTW3bsGGD7urVq4YtMTExy9QWmbK1bt06NbWlcePGass+taVNmzZqipdMV/Hz88txasu7776rjt2MGTNynNpijd+B999/X42iP3funPp8ym2ZcbB69Wr1OI9v/jMe9S14jPOOgfoeZF6efJhkHp4M85c5v6TTrV+/XgXo7Fvv3r0NU7RGjx6tAq18SVq1aqXmqxqLiopSgblYsWJqykXfvn3VCYAxmYPdrFkz9RolS5ZUJwDZ/fXXX7pKlSqp/yOZqiHzYy1ZTsdVNplbrZETnkGDBqkpRfJD1a1bNxXMjZ0/f17Xvn17Nfdc5p++/fbbupSUlLv+H2vXrq2OXbly5bK8hzV/B/r166crW7as+pvkx18+n1qQFjy+BR+oeYzzzkb+eZiaOBERERU89lETERGZMQZqIiIiM8ZATUREZMYYqImIiMwYAzUREZEZY6AmIiIyYwzU9yGrFY0ZM0ZdUv7j8S1YPL4Fj8e4YPH46nEe9X3I8peSplEW75fl6yh/8fgWLB7fgsdjXLB4fPVYoyYiIjJjDNRERERmzOrzUUsKxX379qn0ara2eTsviY+PV5eXL19WTTCUv3h8CxaPb8HjMS5Y1nx809PTVdrNOnXqqBSg92P1fdS7d+9Gw4YNTV0MIiKiu+zatQsNGjRAka5RS01aOxiBgYGmLg4RERGuXr2qKpFajCrSgVpr7pYgXapUKVMXh4iIyCA3XbImHUy2adMmdOrUCUFBQbCxscGiRYuyPC6t8h9//LEKsi4uLmjdujVOnTplsvISEREVNpMG6oSEBNSqVQszZszI8fFJkyZh2rRp+P7777Fz5064ubmhbdu2SEpKKvSyEhERmYJJm77bt2+vtpxIbXrq1Kn46KOP0KVLF3XfL7/8otrzpeb9wgsvFHJpiYiICp/Z9lGfO3cOERERqrlbIyvUNGrUCNu3b2egJqICkZaWhpSUFFMXgyycg4MD7OzsrDtQS5AW2UfEyW3tsZzImrDG68Jq8/CIiO5HWvHktyUmJsbURSEr4eXlhYCAADUGyyoD9cOaMGECxo4dWzAvnpYKrB0LhLQEKmbW9InI8mlBukSJEnB1dX3kH1cq2id9iYmJiIyMVLcfdWqw2QZqOQsRsnKL8R8pt2vXrn3P540aNQojRoww3JYVbapWrZo/hdr1I7BtGhD2P2DABsC7XP68LhGZvLlbC9I+Pj6mLg5ZARcXF3UpwVo+V4/SDG62a32HhISoYL127VrDfbKEnIz+bty48T2f5+TkpLKsaJu7u3u+lWm+bVucdaoCJMUC83oCybfy7bWJyHS0PmmpSRPlF+3z9KhjHkwaqG/duoX9+/erTRtAJtcvXLigmp3efPNNjBs3DkuWLMGhQ4fwyiuvqDnXXbt2LfSyXom5jQ//PYkXYwcjwcEHiDwKLBkibRyFXhYiKhhs7iZz/DyZNFDv2bNHLUgum5Ama7kui5yI9957D0OHDsWAAQPUWqgS2FeuXAlnZ+dCL2uQlws+61od1+CNPglDkG5jDxxZCGz9ptDLQkRERYdJA/Xjjz+uOt2zb3PmzDGcjXz66adqkIcscvLff/+hUqVKJitvj/ql0aN+KexOD8VEm776O2Vw2enM5nkiIksXHBys1rHIrQ0bNqjf64IeMT9nzhw1krqoMds+anP1aZfqqBzgjh8SH8dal7aALh2Y3w+4ec7URSOiIkaC4/22MWPGPHTWQWnJzK0mTZqoJBOy1gXlPwbqPHJ2sMPMl+uhmJMDBka/hMtu1YCkGODPl4E7CaYuHhEVIRIctU1qwDKA1vi+d955x7CvtFampqbm6nX9/PzyNLDO0dExX+YLU84YqB9CiK8bJj1XE3fggGeiBiLZ2Re4dhhYzMFlRFR4JDhqm9RmJVBqt48fP65mvaxYsQL16tVTM2K2bNmCM2fOqGWZZfGoYsWKqfE/0q14v6Zved2ff/4Z3bp1UwG8YsWKapDvvZq+tSbqVatWoUqVKup92rVrp04eNHLSMGzYMLWfTIkbOXIkevfunefBwjNnzkT58uXVyUJoaCh+/fXXLCcn0qpQpkwZ9ffLYGR5T813332n/hYZ9yTH47nnnoM5YqB+SB1qBKJPk2A1uGxA0jDobGVw2QJg23RTF42I8mvRijupJtnkvfPL+++/jy+++ALHjh1DzZo11aDcDh06qKmv+/btUwFUshjKbJv7kYWkevTogYMHD6rn9+zZEzdv3rzn/rLgx+TJk1XglEyJ8vrGNfyJEyfi999/x+zZs7F161Y1/TZ7BsUHWbhwIYYPH463334bhw8fxuuvv46+ffti/fr16vF//vkHX3/9NX744QeVeVFev0aNGobBzBK0ZRzUiRMn1EDlFi1awByZ7YInluCDDlWw/2IMNl6sgO99+mNgwkxg4ySgdk/AjYsmEFmy2ylpqPrxKpO899FP28LVMX9+niUQPfXUU4bb3t7eKmuh5rPPPlMBT2rIQ4YMuefr9OnTBy+++KK6Pn78eJXZcNeuXSrQ50TmDkvmQ6ntCnltKYtm+vTpaoEqqaWLb7/9FsuXL8/T3zZ58mRVrkGDBhlmDu3YsUPd/8QTT6iTA2ldkJwRsva21KwbNmyo9pXHJCPj008/rVoeypYta5iBZG5Yo34Ejva2mNGzLrxcHTAxqhnW+/cG+q1kkCYis1G/fv0st6VGLTVbaZKWZmdplpba9oNq1FIb10iAk/5wbYnMnEgTuRakhawwqe0fGxurVpnUgqaQlbukiT4vjh07hqZNm2a5T27L/aJ79+64ffs2ypUrh/79+6sTEq2fXk5eJDjLY7169VK1e2kFMEesUT+ikl4u+Pr52ug7ezf6hrfFNxHF0UW/+ikRWTAXBztVszXVe+cXCarGJEivWbNG1TorVKiglrqUvtk7d+7c93WkRmpM+qTT09PztH9+NunnRunSpVWztvTBy98sNe8vv/wSGzduVLXosLAw1b++evVqtX6H9GfLiHdzmwLGGnU+eCK0BIY8UUFdH7XgEE5HxgMXdgIrRnJwGZGFksAizc+m2Apy9LT0B0tzsTQ5S3+tNA2fP38ehUkGvsngLQmKxuutS+DMiypVqqi/x5jcNs7vICci0gcvTfUSlCVNsqx0Kezt7VWz+KRJk1TfuxyHdevWwdywRp1P3nqqEvaGR2P72Si8/8t6/J38OmxSEoESVYF6vU1dPCIiRUY5L1iwQAUvOSEYPXr0fWvGBUVWnZRsh1Krr1y5suqzjo6OztNJyrvvvqsGuEnfsgTcf//9V/1t2ih2GX0uJwCNGjVSTfG//fabCtzS5L106VKcPXtWDSArXry46h+X4yAjx80Na9T5xM7WBt+8WBsl3J2w54YdFnr3h65qF6D6s6YuGhGRwZQpU1RgkkVKJFi3bdsWdevWLfRyyHQsGZwmORwk0ZL0lUtZ8rJEdNeuXfHNN9+oZvxq1aqp0d0yilxWvRTShP3TTz+pfmvpY5cALsFcpoPJYxLUn3zySVUzl4Fvf/zxh3odc2OjK+xOg0J26dIl1U9x8eJFlCpVqsDfb+fZKLz0806kpadjQrcaeLFR2QJ/TyJ6NLJEsSQFkqx9psglQFC1WQmYUkOWkejW/rm6lIfYxBp1PmtUzgfvtJGmExt88u9RHL4cq++nDvsFuGOeIwqJiApbeHi4qu2ePHlS9RkPHDhQBbWXXnrJ1EUzOwzUBeD1FuXQqnIJ3ElNx6Dfw5C8ZASwZKh+s+4GDCKiXLG1tVV9yLIymjRNS7CWpmmpVVNWHExWAGxtbfBVj1roOG0LLtxMxLSIGnjH1h42h+cDQXWAJvdeVICIqCiQZt/sI7YpZ6xRFxAvV0fMfLkuHO1sMeOcP7ZXGKF/YM1o4OwGUxePiIgsBAN1AapZygujn9Y347xyuDZulH9Wnxbz775AdLipi0dERBaAgbqAvfxYWXSqFYTUdODZC88h1b8WcPsm8GdPDi4jIqIHYqAuYDJ5f8IzNVDOzw3h8Tq8bfsudK6+QMQh4N/hHFxGRET3xUBdCIo52WNmz3pwdrDF4nO2+KfcOMDGDjj0F7BjpqmLR0REZoyBupCEBrhjfDd9HtR393rgTN0P9A+s/gg4t8m0hSMiIrPFQF2InqlbCi82LK1au7vvq4nEKs8BujTg7z5AzP1TzBERFRRZcvPNN9803A4ODsbUqVMf2K23aNGiR37v/Hqd+5GsWLVr14alYqAuZJ90qoaqgR64mZiCV6Nehi6gFpAYpR8Jzv5qIsoDWau7Xbt2OT62efNmFQQlK1ReSVarAQMGoDCC5dWrV9G+fft8fS9rw0BdyJwd7NT8ancne2y/kIhvS3yiz7DV5jM5tTR18YjIgrz66qsqz7KsG52dJKeoX7++SkaRV35+firbVGGQNJtOTk6F8l6WioHaBMr6uOHL7vovz1e7krCqxT9A2SamLhYRWZinn35aBVVZitPYrVu38Pfff6tAHhUVpbJUlSxZUgVfyUEtWaLuJ3vT96lTp1Q6SEksIbme5eQgp2xYlSpVUu9Rrlw5lT4zJSVFPSblGzt2LA4cOKBq+bJpZc7e9C1LiUpGK0lHKVmuBgwYoP4ejeTSlqxZkjErMDBQ7TN48GDDe+U2Acinn36qkmHISYLU9FeuXGl4/M6dOxgyZIh6ffmbJS2mpOQUksdKWgfKlCmjnhsUFIRhw4ahIHEJURNpVz0QrzULwc9bzuGd+YdQJdALZXxcgSv7gP1/AO0mALZ2pi4mEd1JyPtz7JwAu4yf17RUIC0ZsLEFHFwe/LqObrl+G3t7e5UmUoLehx9+aMjlLEFa8jBLgJYgV69ePRVIPTw8sGzZMvTq1Qvly5dHw4YNcxXUnnnmGfj7+2Pnzp2IjY3N0p+tcXd3V+WQwCXBtn///uq+9957D88//zwOHz6sgqGWK9rT0/Ou10hISFCpLiXtpTS/R0ZG4rXXXlNB0/hkZP369SqIyuXp06fV60uwlffMDUmN+dVXX6m0mJLLetasWejcuTOOHDmi8nVPmzYNS5YswV9//aUCsmS4kk38888/+PrrrzFv3jyVEjMiIkKdgBTZQC0fNDlzkWTfcjDkAyBnUx999FGekoubq5HtK2PfxRjsDY/GwN/34p9+NeD827P6PmuPQKDZW6YuIhGND8r7c7rPAap1018//q9+wGjZZkDfZZn7TK2h/65nNyY2T2/Vr18/fPnll9i4caMhD7M0ez/77LMqGMr2zjvvGPYfOnQoVq1apYJQbgK1BNbjx4+r58hvsBg/fvxd/cryu2xcI5f3lGAmgVpqx5JvWk4spKn7XubOnatSQ/7yyy9wc9OfsHz77beqL37ixInqZEFIPm25387ODpUrV0bHjh2xdu3aXAdqqY3LicsLL7ygbstrS9CXVoQZM2bgwoULKmA3a9ZMxRqpUWvkMfkbWrduDQcHBxXIc3McrbbpWw7ezJkz1X/IsWPH1O1JkyZh+vTpsAYOdrb49qU68HZzxJErcfh4ZTh07b8EgpsDDV4zdfGIyAJIoGrSpImqFQqpYcpAMmn21io8kt9Zmry9vb1VwJSgKwEnN+S3VxJoaEFaSI03uz///FNlwZIgJu8hgTu372H8XrVq1TIEadG0aVNVqz9x4oThPqnJSpDWSO1aat+5ERcXhytXrqjXNSa35f2FVAj379+P0NBQ1ay9evVqw37du3fH7du3VfO+nBgsXLgQqampKLI16m3btqFLly7qbEk7S5O+lV27dsFaBHq6YOrztdFn9i78tecSynjXxJBXlkgKrsydZDS4FbQgEFmkD648XNO3pnIn/WtI07exNw8hv0hQlpqy1AalNi3N2i1btlSPSW1bmnqltijBWoKgNF1LP2x+2b59O3r27Kn6oaXpWmrxUpuW5uWC4ODgkOW21HolmOeXunXrqtzYK1asUC0KPXr0UDXo+fPnq5MWOWmQ+6WvftCgQYYWjezlKhI1ajlLlOYMSSwupB9gy5Yt9x3Kn5ycrM6YtC0+Ph7mrkUlP4zpXE1dn7z6JBbsN/ph2PwVsPxdTt0iMhXpM87rpvVPC7ku9xn3T9/vdR+CBBLJ7yxNx9JsLM3hWvegpJKUCs/LL7+saqtSE9R+U3ND8kNL/6xMo9Ls2LHjrkqVNA9LP7mMNJdm4/DwrImHHB0dVe3+Qe8lv/PSV63ZunWr+tukdpsfpJ9eWgeyp9iU2zJQzng/6fv+6aefVGuB9E3fvHlTPSZN+dIcL33ZGzZsUCcq0i9fJGvU77//vgq20rQjzRzyn/z555+rM7d7kZF5clZnaV5pHIzL0bfxw6azeG/+Qfh7OKOp+zVg7WdSpdYPLGv3BWvWRHQXaWqWoDJq1Cj1mylNtxoJmlITlGAqfbtTpkzBtWvXsgSl+5GapIzm7t27t6o5yutLQDYm7yHN3FKLbtCggRqwJk3CxqRFVGqp0qQso61loFn2aVny2/7JJ5+o95LxSdevX1ctBTL4Teufzg/vvvuueh9peZBBaNIKIeX6/fff1eNyjKQ5XQaayUmCDM6TJn0vLy81qE1iUaNGjdQIdxlDJYHbuB+7SNWoZbCDHDg5SwwLC8P//vc/NQhALu9FPqgyKlHbjh49Cksxsl1lPF0zEKnpOrzx614c15UGOmf0x+/8Hlj1IWvWRHTP5u/o6GjV9Gzcnyx9xdKUK/fLYDMJODK9KbckUEnQlX5ZGTQlo7ClwmRMRky/9dZbanS2BD45KZDpWcZkcJsszvLEE0+oKWU5TRGTwCf951JzlYD/3HPPoVWrVmqcUn6SfucRI0bg7bffVt0BMhpdRnnLCYeQkwgZDyWtA1KO8+fPY/ny5epYSLCWWrb0acscdWkC//fff9U0sYJio5NJYWZK+gKkVi1z5DTjxo1TZzAyCjE3ZCEAeR1pupGzOHOXlJKGV2btwq5zNxHo6YwFg5og8PSf+kxboslQ4CkujkKUn2SksdT2QkJC1LxZooL+XOUlNpl1jToxMVGdwRiTJvD8HDRgjiuX/dirHsr7ueFqbBL6zt6N+Go9gY5T9Dtsmw6sHcuaNRFREWHWgVo666WJRfo7pOlBml+k76Bbt4z5iVbKy9URc/o2hJ+7E45HxGPgb2G4U6cv0GGyfoctXwPrxjFYExEVAWYdqGW+tPRRyPB3GQ0oE+hff/11NSfQ2pX2dsXsPg3g6miHLadv4P0FB6GTudXtJup32DwZ2KBf0o6IiKyXWY/6lg59mfv3oHRr1qp6SU/M6FkXr/1vDxaEXUYpLxeMaPOGPjXmqg+AjRMBGzvg8ZGmLioRERXFGjUBT4SWwOddq6vr09adxrxdF4DGg/UDysSG8cCmjCZxIiKyOgzUFuCFhmUw9MkK6vqHiw5j/YlIoOkwoPUY/Q7rPgOu5j3nLBFlZc0DVclyP09m3fRNmUY8VQmXY26rJvDBv4fhr9cbo7ok7dClA66+QGDec84SUeaqWTLDRNaAljm+ctsaEv+QacisZ1miVRZskc+VfJ4eBQO1hZAfjS+eqYnIuGQ1uKzvnN1YMLAJSjd/O+uOqcmAPZOwE+WF/JjKXFdZJlOCNVF+kAVcJLtW9mnGecVAbUEc7W3x3ct10eP77WralgTrf95oAk/XjIXgE24Av3QB6vQCHnvD1MUlsihS65EfVcmE9KA1qYkeRNb8kLSe+dEyw0BtYTycHTC7bwN0m7ENpyNvof+ve/Drqw3hZG8HHP4HuHZYP8+69ouA892J2Yno3uRHVTIgFVQWJKKHwcFkFpoac06/BnB3sldLjb791wGkp+uAhgOAVp8AfZYxSBMRWQkGagtVOcAD3/eqBwc7Gyw9eBUTVx7Xr//dfATgqx8hrsRfM2UxiYjoETFQW7CmFXwx8Vn9aG9Jj/nL9vNZdzj1H/BNLWDfb6YpIBERPTIGagv3TN1SePupSur6mCVHsOaoUQ367Hog9TaweAjwS1dg7/+ARH3icyIisgwM1FZgyJMV8EKD0pBu6qF/hGHfhWj9A23GAY8Nkll9+qD97zBgckXgt+eA/XOBpFhTF52IiB6AgdpKRqqO61odj4f6ISklXa0NHh6VoO+zbjcBGBoGPDka8K8BpKcCp9cAiwYCX1YA5r4AHPwLSI439Z9BREQ5sNHJEipWLC/JuS1dQnIqnv9xOw5fjkOIrxv+GdgE3m7ZVsS5fhI4shA4sgC4fjzzfjsnoOJTwNNfA8VKFHrZiYiKkkt5iE2sUVsRNyd7zOrTACW9XHDuRgJe+99uJKVkW7jBr5I+29bgncDA7UCL9wCfCkBaMhC+FXApnrlv5DEg5Xah/x1ERJSJgdrKlHB3xv/6NYCniwPCLsRg+Lx9SJPO65z4VwWe/BAYsgd4fTPQaRpgl7HQgzS0/N5D3zx+aW+h/g1ERJSJgdoKVSjhjp9eqQ9HO1usOnINny09qhaJvyfpy5akHlU7Z94Xf1U/CE2eV6JK5v3HlwOn1gBpKQX7RxARkcIlRK1UwxBvfNWjFob+sQ9ztp1Xg8tGdaiCSv7uuXsBjyBg+EEg+hzg6Kq/T4L2f2OAGyf0TeRVOgGlHwNs7QFbO/1mk/3SFgiokdnvLdPDbp4DnD0A34qZ7yf3yYmBep69vmYvWcEecTF7IiJLx0BtxTrVCkLUrWSMW3YM609cx8aT19G9XmmMaFMJ/h7OD34BCZI+5TNvp90BQloAt28CCdeBsF/024N0nwNU66a/fnYDML8vENwc6LM0c5+fntS/rjEnDyCwVsZWGwiqDXiXZ/AmoiKFgdrK9WkaghaV/DBp5QmsPBKBP/dcxJIDV9C/eQgGtCyPYk55+AhI+syOk4H2E4HzW4Cji4DocECXBqSn6XNjq8s0o8t0wNnL6DWcAc8yd48sdyymPxGQ6WPyXLlMjgPOb9ZvxvsF1NQHbckSJv3sRERWjNOzipA9529i/PJjapCZ8C3miOGtK6nFUhzszKyWKn3g108AV/cDV/brLyMO61da07y8AKjQSn/93Gbg+FKgQmv9NDMiotySMJiaBNxJAO7cyrjUridmXnf1Aap1RWHHJtaoi5D6wd5qbvXKwxEqicf5qESMXnQYs7eew/vtKuOpqv75kjs1X0gfdUB1/VbnZf19aanAjZOZwTuoTub+p/8Ddn6v/7JpgTolCVjzsb7pXGrgvqGAHT/yRFYVYJPjgcQo/fgXdRkFJMUAHiUzB8jKftLllnwLeOZHwNVbf//aT4FdP+mDsLQIPkiZxvkWqPOCv1pFjATi9jUC0bqqP+buvIBv1p7C2esJGPDrXjQM9saoDpVRp4zRXGpzIkFWmrplq/1S1sfKP5HZh66JPArs+iFrs7t/df0odhcvfR+4k3u2LaNfXJumRkSFR7rKZMaJBFv5rmrjUY4s1He3aYHYOCjL9z4nFZ7KDNRSATm5GkhJ0C+drAVq6WaTLjZjDq6Ao1vGZTH9dW0zngFTiNj0XcTFJaXgh41n8PPmc0hO1Z9RdqwZiPfahqKsjxssWtQZYM+sjKbzA8CdXC6T+v6FzHze/74JHJoPPPEB0FjWTQcQfV5fU9cCu1zKF1q7lFHyDi6Ag3zZXTK+9C5AMX/9SHgiawmq0oIln22tJe7mWSA+AkhJ1C+WJJs0G6vrRvfJdblfBpAG1dWv5yBS7wDj/PTX3zuXGVCXjgD2/N+9yyJBVZqlZX+5lHExcsLd7M3MfWTgq8xCkdkq2vdb0gBLbVoLxPI6hfQdZdM35ZqHswPebVsZLz9WFl+tPol/wi5h2cGrWH0kQt039MmKdy9DailkxHrbzzN/VORHRJrN5VLOoqXJ7K4tTh9sNXL2LQFevuAa+SE6ujjv5XnzMOBVWn993Tgg7FfgsYGZPybyukvfygjy2ll9RsCX4K9+TDJOCAwnB8UAj1KAvYX+H1mb1OSstT3jGqAEJcP6Azqgckf9mAoRcwHYOFEfYLTPrNgwUf951dY0eOAlgND2mS1OCVHAkiH6KY/P/5r1dS/vufu5Ob2ulFkCa8W2mQFVvhdflNFf/yhSP9BUve4XwME/83bMjOuK8jl28da3aMn3UQvUFdtkBGKfrAFZ27QppPdT95W773P3l5WfYO7MPlBfvnwZI0eOxIoVK5CYmIgKFSpg9uzZqF+/vqmLZlUCPV0wuXstvNosBBNWHMemk9cxe+t5zN97CYMer4C+TYPh7GDBtUFpQvOtoN/youNXwJMfZV1a1asM0GHy3cE+KU7ftKZqEVJ70LaMWoUEWo38eN+K0NdIDPfdBE4sz/vf9sZWfV++2D4D2DFT/0MtrQBC+uVWvJc1uBu3AEiXgIyyV6P0tVH3afqBetoPpbRIXNihX25WG8AntZ/NXxk9z+i52m35wZV15O2NtiqdM6f9xVzUnzy5BwKl6mddk15qNlI27XnyOvJ6hTWOQsZE3I7Wv7fM+9daU6SFRY7bY29k7vt/bYBrR3PfaiM8S2UGavk8SN5496CsgfrUan1AzQvPjJNBIYMv5TNll+1E7kqY/rXzwsdo3QM5kdTIZ1wL1LL+gnxGjFuUZF91aXRdOwmVoOwdkvV93jt79/9xaDv9VkSZdaCOjo5G06ZN8cQTT6hA7efnh1OnTqF4cTPtQ7UCVQI98Eu/hth86jrGLz+OY1fj1MCzX7efxzttQ9G1dknY2prJgLPCoM7cM4KVRn6MGvbP2+tk72FqORKo1wdwy2jmE+4BQKdvcg7ycl0CrjTTyUmBdin3SeDV3LoGxF7U36+RgTX7f0eevbY2828/uxFYMxqo+UJmoJYAvfGLvL+uX+XMQC39joveAMo/CfRamHVefY5BzyYzaMuPudqktcNGP22wxnOZ5ZUMcbLYzktGNbzZHYH4K5nPkUvj15BLOdbagCQhJ2Xa/3fsJWDdZ/pgZByo1cjgjPLKoj1ZanwZ1+VkTwXMjHKXaZL5fAnQkuFOTp6MNXodiO+SEbhs7r5U72d8H/R/s0Zq6PKZMm4RUq/7hr4J+L6vlXEpJ0cSYGVwlkbue+d0RjePUdBuPUa/PQpzGdBqRsw6UE+cOFG14UsNWhMSku3siwpE84p+WDrUF4v2Xcbk1SdwJTYJI/46oPqyP+hQBc0q+pq6iJYl+4+PBGXZjMkPugTvRyH5x6t0yXpyIbWXVh8bBXq51FoCbukTstg6ZKwuZ5+50pxxC4BvJf2iNSXrZd4nz6n/qtFzbI2u2+sDltSqpdVAmoTlfeTSuMYn5SzdCPDLNkhHO/mQ52QZLJQxjca4JUIjr62RE5y4y/pxAcZiwvUnMnkhx8y4NUVmIchaAMae/TljNT1vwMkz74vySBNsi3fuvr9mDzwSOY45faZk8OWjKmZ0kklFdzBZ1apV0bZtW9XpvnHjRpQsWRKDBg1C//65r81wMNmjkwxcs7aew8z1ZxCfnKrua1nJD++3r6xq4EQFSsYXaEHeEPDvZPSjputbK+TSIzCzi0L6UKVvV2p7fqGZr3U5LCOgZ/TBqik52a5Lc7tWG5YaKaf0UQHIS2wy60Dt7Kxf5nLEiBHo3r07du/ejeHDh+P7779H7969c3xOcnKy2oz7uCXgM1A/upsJdzBt7Sn8tiMcqek6VUlsVdkftUt7qoAtW6Cns/nMxSYiMlNWE6gdHR3VoLFt27YZ7hs2bJgK2Nu3b8/xOWPGjMHYsWPvup+BOv+cv5GAL1edwLJDkmErKy9XB1QJ0AftKoHu6rKifzE42VvwQDQionxmNdOzAgMDVW3YWJUqVfDPP//c8zmjRo1SNfDsNWrKP8G+bpjRsy4GXo7FtjM3cOxqvBp0djryFmISU7D9bJTaNPa2NqhQoliW4C2bb7GMkaJERGSZgVpGfJ84cSLLfSdPnkTZsmXv+RwnJye1aeLisq06Q/mmeklPtWmSU9Nw6totFbS14H30ahxib6fgeES82hbuy3y+n7sTqmYEbQngcj3E1w325rbuOBGRpQVqqapLP6RWXd+1axfmzp2raq4DBgzIt8K99dZbaNKkCcaPH48ePXqo9/nxxx/VRuZHmrezB2/pWbkam5QRvPUBXIL3+agEXI9PxsZ4ffrNzNewRWiAPmi3qeaPlpVKwK4oTQcjIsqPPurmzZurgNyrVy9EREQgNDQU1apVU3Ochw4dio8//hj5ZenSpao5W15bpmZJszZHfVu+hORUnLiWUeu+og/iUuNOvJOWZb+SXi54qVEZPN+gNJvKichqFPhgMllwZMeOHSpAT5s2DX/++Se2bt2K1atX44033sDZs7LknXlgoLYc6ek6XLiZqIL27vPRWLDvkurzFg52NmhfPRC9GpdF/bLFObKciCxagQ8mS0lJMfQD//fff+jcWZ+hpHLlyrh69e6RwES5ISueyUA12STD13vtQrH04FU1HWz/xRgsOXBFbaH+7ni5cVl0q1MSxZzMepgFEdEje6hRO9LMLXOZN2/ejDVr1qBdO/0arFeuXIGPj8+jl4pI5tE72OG5eqWwaHBT/DukGZ6vXxrODraqyVzyaDf6/D98tOgQjkdwwCARWa+HavresGEDunXrpkZUy8Ijs2bNUvd/8MEHOH78OBYsWABzwaZv6yIjyP/Zewm/7QxXebQ1DYKLq2xf7aoHcM42EZm9QlnwJC0tTQVq4wQZ58+fh6urK0qUKAFzwUBtneRju/1MlArYq45cQ1q6/mPs4+aoBp692LAMSnvnIvUdEZE19lHfvn1b/VBqQTo8PBwLFy5Ui5HI2txEBU0GkzWp4Ku2a3FJmLfrIubuCse1uGR8t+EMZm48gydDS6hadotKfpziRUQW66Fq1G3atMEzzzyjRnjHxMSoQWQODg64ceMGpkyZgoEDB8JcsEZddKSmpeO/Y5Fq8NmW0zcM95cq7oKejcqiR/1S8OEULyKysNj0UIPJwsLC1FxqMX/+fPj7+6ta9S+//KKmaxGZgqxoJn3Uv73WCOvebolXm4XAw9kel6Jvq5zajSesw5vz9mFveLSpi0pElGsPFagTExPh7q5PcC5zp6V2bWtri8cee0wFbCJTK+dXDKOfroqdH7TGpOdqomYpT9xJS8ei/Vfw7MxtGDw3TDWZExFZZaCuUKECFi1apKrsq1atUk3hIjIyEh4ezE9M5sPF0Q496pfGkiHNsGRIUzXdS7qrlx28ilZfbcTsrecMA9GIiKwmUMsSoe+88w6Cg4PRsGFDNG7c2FC7rlOnTn6XkShf1Czlhcnda6mgXbu0F24lp2Lsv0fRZcYWHLgYY+riERHl7/QsWeNbViGrVauWavYWkjRDatQyuMxccDAZ3Wu50j92X8DEFccRl5QKWZH05UZl8U7bUHi6OJi6eERk5S4Vxjxq4zcT5hoEGajpfiSD14Tlx7Bg32V1WxJ/jH66CjrXCuJ64kRkuaO+09PT8emnn8LT01PlhpbNy8sLn332mXqMyFJITuwpz9fG3NcaoZyfG27cSsbwefvx8v/txNnrt0xdPCKihwvUH374Ib799lt88cUX2Ldvn9okZ/T06dMxevTo/C8lUQGThVNWDG+Od9pUUjmxt56OQrupmzFlzUkkpWRNvUlEVJgequk7KChIJeXQsmZpFi9ejEGDBuHyZX0zojlg0zflVXhUAj5efAQbT15Xt8v6uOLTLtXRspKfqYtGRFaiwJu+b968meOAMblPHiOyZGV93DCnbwN817Mu/D2cEB6ViN6zdnHuNRGZxEMFahnpLU3f2cl9NWvWzI9yEZmUDCTrUCMQ/41oiX5NQzj3mogsq+l748aN6NixI8qUKWOYQ719+3ZVhV++fLlheVFzwKZvyg+HL8fiw0WHDfOtq5f0wOdda6BWaS9TF42ILFCBN323bNkSJ0+eVDmpJSmHbLKM6JEjR/Drr78+bLmJzFb1kp5YMLAJxnWtDndnexy+HIeu323F6EWHVY5sIqKC8sjzqI0dOHAAdevWVbmqzQVr1FQQc6/HLz+GhZx7TUTmWqMmKupzr7/W5l77Zp17ffBSjFr1jIgov9jn2ysRFcW51282x48bz2L6+tNq7nXnb7fCt5gjmlXwRYtKfuqyhIezqYtKRBaMgZroETjZ22Foq4roXDsIk1aewLrjkbhx645KpymbqBzgroJ284q+aBDsDWcHO1MXm4isNVDLgLH7kUFlREV17vWMnnWRnJqGsPAYbD51HZtP3cDhK7E4HhGvth83nVWrnjUM8UaLin5oXskXof7u7NcmovwL1LK294Mef+WVV/LykkRWV8NuXN5Hbe+1A6JuJWPrmShsPqkP3BFxSepSNizX93dLTVsCd9MKvuo2EVGBjfouaLK2+KhRozB8+HBMnTo1V8/hqG8yF/JVOx15C5tUoL6OHWejkJSSNYlN1UAPVdOWwF0/uLgK/ERkffISmyymj3r37t344YcfuPIZWSxp4q7o7662V5uFqGQfYeHRhsB95Eocjl7Vbz9sPAtnB1s8Vs4HzaWZvKIvKpYoxmZyoiLIIgL1rVu30LNnT/z0008YN26cqYtDlC9kUJmMHJft/faV1TSvradvqGQg0jQu87U3nLiuNlHC3Uk1j+s3HwR6upj6TyCiQmARgXrw4MFqydLWrVs/MFAnJyerTRMfH18IJSR6dLJwSpfaJdUmzeQnrsVj88kb2HTqOnadu4nI+GS1yIq20Irkz26WEbil5u3p4mDqP4GIimKgnjdvHsLCwlTTd25MmDABY8eOLfByERUkaeKuHOChtv4tyhmaybecvqEGpx26FIOz1xPU9sv2cJU0pEYpLzQt76OCd92yxTkNjMhKmPVgMulkr1+/PtasWWPom3788cdRu3btew4my16jltzYVatW5WAysiqxiSnYfjYK287cUMFbArYxmQYmc7alti2Bu2qQB+wkmhORxQ0mM+tAvWjRIpX4w84us2Yg64hLbcPW1lYFZOPHcsJR31QUXI29rVZGkz5u2aSZ3Jg0izcp76P6wyVwB/u4cmAakQlZTaCW/uXw8PAs9/Xt2xeVK1fGyJEjUb169Qe+BgM1FdVpYKqZ/HSUmgZ2Kzk1yz4lvVxU4G5WUR+4fYpx/jZRYbKa6Vnu7u53BWM3Nzf4+PjkKkgTFfVpYH2bhiA1LR0HLsVi22l9M3nYhWhcjrmNv/deUps0k49qXxmvNA6GLZvHicyOWQdqInp09na2qFe2uNpkXfLEO6nYfT5aPxXsxHU1unzMv0fx37FIfNm9Jqd9EZkZs276zg9s+ia6N/n6/7ojXOXXllXSPJzt8VnX6mqKGBEVHOajJqJcN5NLk/eyYc1Rq5Qn4pJSVW7tIXPDEJN4x9TFIyIGaiIS5f2KYf7AJnizdUU1jWvpwatoO3WTWiWNiEyLgZqIFAc7W7zZuhIWDGyiVj27FpeM3rN24ePFh3H7Tpqpi0dUZDFQE1EWtUp7YdnQ5ujduKy6LSufdZy2GfsvMt88kSkwUBPRXVwc7TC2S3X8+mpDBHg44+yNBDw7cxu+XnMSKWlZU3MSUcFioCaie5IUm6vebIHOtYKQlq7DN2tPqYAtC6oQUeFgoCai+/J0dcC0F+uoTaZvHbwUq5rC52w9h/R0q57dSWQWGKiJKFekVr36rZZoXtEXyanpapGUV2btUuuME1HBYaAmolwL8HTGL/0a4tMu1eDsYKuWJG379SYs3q/PkU1E+Y+BmojyhIukEBUuBmoieihcJIWocDBQE9FD4yIpRAWPgZqICmyRlP+OXsOdVM67JnoUTHNJRPm6SErrqv549++DapGU137Zo6Z0PVU1AB1rBqBZBT842rN+QJQXDNREVCCLpMjiKEsPXkFkfDL+CbukNncVtP3RoXogmlfyhZO9namLS2T2mI+aiAqMrGa2Nzwayw9dVZsEbY27k72qfXeoEajmZjs7MGhT0XEpD7GJgZqICoWsYrb3QjSWHbyKFYevqoFnmmIStKuUUEG7RSU/Bm2yepcYqDMxUBOZZ9AOk6B96CpWHIpARFxSlqDdKiNot2TQJivFQG2EgZrI/IP2votS045QNe2rsZlB283RDq2q6JvHHw9l0CbrwUBthIGayNKCdozqz15x6CquZAvaT1bxR8caAXg8tASDNlk0BmojDNRElhu091+KwXLVpx2ByzGZyT9cHOxQP7g4Gpf3QeNyPqhR0hP2dpz2RdYZmzg9i4jMkq2tDeqWKa62DztWwYFLsaqmLYPRJGhvPnVDbVq/dgND4PZF1SAPtawpkTVgoCYii0gEUru0l9pGta+ME9fisf1MlNp2nruJ2NspWH/iutqEzNduFOKNx8r5qOBdJcBDBX4iS8RATUQWF7QrB3iorW/TEDVX+9jVOOw4qw/cu87dRHxSKv47Fqk24eXqoAK3NJM3Lu+LSv7F1OsQWQKzDtQTJkzAggULcPz4cbi4uKBJkyaYOHEiQkNDTV00IjIT0sRdvaSn2l5rXg6paek4ciUO2zMC9+7zNxGTmIJVR66pTfi4Oara9mMZfdzl/dwYuMlsmfVgsnbt2uGFF15AgwYNkJqaig8++ACHDx/G0aNH4ebmlqvX4GAyoqItJS0dhy7HqqAttW4J3EkpWROF+Lk7qYAtwVv6ukN83Tg4jQqU1Y76vn79OkqUKIGNGzeiRYsWuXoOAzURGZNsXgcuxRj6uGW1tOwZvhztbFG+RDGE+hdDqGpmd0elAHcEeTqz5k35wmpHfcfGxqpLb29vUxeFiCyUZO9qEOyttmGtKiIpJQ37LsSopvIdZ6Jw+EosEu+kqX5v2YArhufKILVQf3eEBmRs/u6qr9zT1cGkfxNZN4upUaenp6Nz586IiYnBli1b7rlfcnKy2jSXL19G1apVWaMmolzP35bpX8cj4nHyWry6PBERh7PXE5CanvPPpb+Hk6HmrQXyCiWKcVEWKlo16sGDB6v+6fsFaW0A2tixYwutXERkXWQaV2lvV7VJSk6NNI+fvXELJ1Tg1m8SxCWoS4KRa3HXsenk9czXsQGCfd0MgVuCeNMKvnB3Zu2brLBGPWTIECxevBibNm1CSEjIffdljZqIClN8UgpOXtMCeJya4y3XoxNT7tpXFmbpXr8U+jYJQRkfV5OUl8yD1dSo5Rxi6NChWLhwITZs2PDAIC2cnJzUpomLkz4mIqKCITXkemWLq834t+t6fLIhaEvNW/Jyn7uRgNlbz+N/286r2vqrzcqpUeYcoEYWG6iluXvu3LmqNu3u7o6IiAh1v6enp5pXTURkjiTwlvBwVlvzin6G4L3x5HXM2npeNZFr87plnfJXm4WoDGEy0I3Iopq+73WWOXv2bPTp0ydXr8HpWURkbmSQ2uyt57Ag7DKSM6aGyYC0VxoH46WGZVDczdHURaQCZrXzqB8GAzURmauoW8mYu/MCftkRrprKhbODLZ6pWwr9moaokeNknRiojTBQE5G5S05Nw9IDV/F/W87hqJq7rfd4qJ9qFm9WwZf92FbGagaTEREVBU72dni2Xik8U7ekygYmAfu/Y9ew4cR1tckUr37NgtGldknOzS6CWKMmIjJD528kYM628/hrz0W1UpqWTKTnY2XR67Gyan1yslxs+jbCQE1Elkxybf+5+wL+ty1cLa6irUXeqVaQahavGuRh6iLSQ2CgNsJATUTWQNJ3rjwSoZrFZW1yjWT96tM0GM0r+sLVkb2ZloJ91EREVkbSbj5dM0htYReiMWvLOaw4HKHPu302Cg52NqhTpjialvdFs4o+qFnKCw5M1WkVGKiJiCxM3TLFUfel4qop/Jdt57H04FV1fde5m2r7+j/AzdEOjcr5qPXFm1bwUQPSOHLcMrHpm4jIwsnP+IWbidhy+ga2nY7CtjM37lpr3LeYE5qUl8CtD96linOtcVNi0zcRUREiNeWyPm5q69morErVKfOxJWBvPR2latk3biVjyYErahNlfVz1te3yvmhc3gfeXA3NbDFQExFZYarO6iU91TagRXmVonPfhWhsPX0DW89EYf/FGIRHJSI86oJaGU1axKsGeqjALbXuhiHeHJhmRtj0TURUBFNzSi1batsSvCXLlzFtYJqsiCZBWxKHuDkxcOcnNn0TEdF9U3O2quKvNhEZn4TtZ/RBW4K38cA0YWsDVCzhjlqlPVGrtBdqlfJCaIA7R5UXEgZqIqIiroS7s1qeVDZpZJVm8a1n9APTpMn8SmySPrf2tXj8teeSeo6Tva1qWpegLQG8dmkvlPF25cjyAsBATUREBhJog33d1CYD00RkXBIOXIrFgYsxOHApRvVxxyelYm94tNo0Xq4OGYHbC7Wl9l3KCz7FuNTpo2KgJiKi+yrh4YynqsqmbyqXUeXnoxJU0D5wMVYF7qNX4hCTmIKNJ6+rTVOquIs+cGcE8OolPThQLY94tIiIKM+jysv5FVNbtzr6gVAysvx4RJyqde+/GKuC+OnIW7gUfVttyw5e1T/XBqjk765q2+X89FPKZKqYbAzgOeNRISKiR+Zob6uWLZWtV2P9fXFJKTh8KRb7Vc1bX/uOiEvC8Yh4tWUni7IE+7iijARub30AL+PjimAfNxR3dSiy/d8M1EREVCA8nB3QROZmV/A13BcRK/3dMTh8ORbnoxJxISoB4TcTVbO5LMoi2x6jfm+Nu5O9PoBL8PZ2ywzoPm4I9HBWtXxrxUBNRESFJsDTGQGeAWhbLSDL/bGJKQi/maBGnMtyqOESwNWiLImqFh6fnIojV+LUlp2k/Szl7aJq3jLyXLYgLxfVPy6Xll4bZ6AmIiKT83R1QE1XfdN5dkkpabiogneiqn1rQVwC+qXoRNxJS8fZ6wlqy4mzg60K2CUzNrlufFtOHqTp3lwxUBMRkVlzdrBDRX93tWWXlq7DlZjbGUE8AReiEnExOhGXY5LU/dfjk5GUcv9ALpVtv2JOKJlRA1fB3NNZf724/rani+lq5QzURERksexsbVDa21VtzZDZF65JTk1T/eKy2trl6Nu4khHAr8Tqb8v9yanpiIxPVtu+CzE5vo+ro50K3NWDPDD1hTooTAzURERktZzs7QyZxXIiK7HdTLijArgK5hLEjTapmcsAt8Q7aWq6mSnWPGegJiKiIsvGxkatniZbjVKeOe4jfeRXY/U1cVM0fjNQExERPaCPPMTXTW2mYL7D3IzMmDEDwcHBcHZ2RqNGjbBr1y5TF4mIiKhQmH2g/vPPPzFixAh88sknCAsLQ61atdC2bVtERkaaumhEREQFzuwD9ZQpU9C/f3/07dsXVatWxffffw9XV1fMmjXL1EUjIiIq2oH6zp072Lt3L1q3bm24z9bWVt3evn17js9JTk5GXFycYYuPv3s9WSIiIkth1oH6xo0bSEtLg7+/PrWaRm5HRETk+JwJEybA09PTsEktnIiIyFJZ3ajvUaNGqT5tzcWLF1G9enVcvapPsUZERGRqWkxKT0+37EDt6+sLOzs7XLt2Lcv9cjsgIOuC7honJye1aRITE9Vlw4YNC7i0REREeSPxrEyZMpYbqB0dHVGvXj2sXbsWXbt2NZx9yO0hQ4bk6jXq1KmjpnNJc7n0bz8K6e+WpvSjR4/C3f3uNWfpbjxmecdjlnc8ZnnHY2baYyaxTIK0xKgHsdHJ+mlmPj2rd+/e+OGHH1SteOrUqfjrr79w/Pjxu/quC5oMTpN+79jYWHh4eBTqe1sqHrO84zHLOx6zvOMxs5xjZtY1avH888/j+vXr+Pjjj9UAstq1a2PlypWFHqSJiIhMwewDtZBm7tw2dRMREVkTs56eZW5kkJqskGY8WI3uj8cs73jM8o7HLO94zCznmJl9HzUREVFRxho1ERGRGWOgJiIiMmMM1ERERGaMgToPmBc792TN9QYNGqhFAUqUKKEWrDlx4oSpi2UxvvjiC9jY2ODNN980dVHM2uXLl/Hyyy/Dx8cHLi4uqFGjBvbs2WPqYpktyZ0wevRohISEqONVvnx5fPbZZ+BQpaw2bdqETp06ISgoSH0PFy1alOVxOV4yZTgwMFAdR0kUderUKRQUBupcYl7svNm4cSMGDx6MHTt2YM2aNUhJSUGbNm2QkJBg6qKZvd27d6sFfmrWrGnqopi16OhoNG3aFA4ODlixYoVaLeqrr75C8eLFTV00szVx4kTMnDkT3377LY4dO6ZuT5o0CdOnTzd10cxKQkKC+o2XyllO5JhNmzZNpV3euXMn3NzcVDxISkoqmALJqG96sIYNG+oGDx5suJ2WlqYLCgrSTZgwwaTlshSRkZFyyq7buHGjqYti1uLj43UVK1bUrVmzRteyZUvd8OHDTV0kszVy5Ehds2bNTF0Mi9KxY0ddv379stz3zDPP6Hr27GmyMpk7ALqFCxcabqenp+sCAgJ0X375peG+mJgYnZOTk+6PP/4okDKwRl1AebEpK1lyT3h7e5u6KGZNWiE6duyY5bNGOVuyZAnq16+P7t27q+4VWTP5p59+MnWxzFqTJk1UroSTJ0+q2wcOHMCWLVvQvn17UxfNYpw7d06tkmn8HZVlRaU7tKDigUWsTGbOebFlzXF68OLz0tcqzZSScpRyNm/ePNWtIk3f9GBnz55VzbjSJfXBBx+o4zZs2DCVzEfyA9Dd3n//fbVedeXKlVVmQvld+/zzz9GzZ09TF81iREREqMuc4oH2WH5joKZCqSUePnxYnblTziRv+vDhw1V/vgxWpNydAEqNevz48eq21Kjlcyb9hgzUOZOERr///jvmzp2LatWqYf/+/eokWgZN8ZiZLzZ9F1BebNKTNdqXLl2K9evXo1SpUqYujtmSrhUZmFi3bl3Y29urTQbkyYAVuS41H8pKRtxKykFjVapUwYULF0xWJnP37rvvqlr1Cy+8oEbI9+rVC2+99ZaapUG5o/3mF2Y8YKDOY15sjZYXu3HjxiYtm7mSMRgSpBcuXIh169ap6SB0b61atcKhQ4dUDUfbpLYoTZJyXU4UKSvpSsk+5U/6XsuWLWuyMpm7xMRENb7GmHy25PeMckd+yyQgG8cD6U6Q0d8FFQ/Y9J1L0g8mTUPy46nlxZYh/H379jV10cy2uVua1xYvXqzmUmt9NzLoQuYdUlZyjLL338uUD5kfzH79nElNUAZHSdN3jx491LoGP/74o9ooZzI3WPqky5Qpo5q+9+3bhylTpqBfv36mLppZuXXrFk6fPp1lAJmcMMtgWDl20l0wbtw4VKxYUQVumZsu3QeyXkSBKJCx5FZq+vTpujJlyugcHR3VdK0dO3aYukhmSz5aOW2zZ882ddEsBqdnPdi///6rq169upoaU7lyZd2PP/5o6iKZtbi4OPWZkt8xZ2dnXbly5XQffvihLjk52dRFMyvr16/P8ferd+/ehilao0eP1vn7+6vPXqtWrXQnTpwosPIwexYREZEZYx81ERGRGWOgJiIiMmMM1ERERGaMgZqIiMiMMVATERGZMQZqIiIiM8ZATUREZMYYqImIiMwYAzUR5TsbGxssWrTI1MUgsgoM1ERWpk+fPipQZt/atWtn6qIR0UNgUg4iKyRBefbs2Vnuc3JyMll5iOjhsUZNZIUkKEsqPuOtePHi6jGpXc+cORPt27dXmczKlSuH+fPnZ3m+pNx88skn1eOSwWvAgAEqo5CxWbNmqQxM8l6SG1rSmhq7ceMGunXrBldXV5VlaMmSJYbHoqOjVQpPPz8/9R7yePYTCyLSY6AmKoIkLd+zzz6LAwcOqID5wgsv4NixY+oxSd/atm1bFdh3796Nv//+G//991+WQCyBXlKZSgCXoC5BuEKFClneY+zYsSr95MGDB9GhQwf1Pjdv3jS8/9GjR7FixQr1vvJ6vr6+hXwUiCxEgeXlIiKTkFR8dnZ2Ojc3tyzb559/rh6Xr/0bb7yR5TmNGjXSDRw4UF2XVJHFixfX3bp1y/D4smXLdLa2trqIiAh1OygoSKVHvBd5j48++shwW15L7luxYoW63alTJ13fvn3z+S8nsk7soyayQk888YSqpRqTpPeaxo0bZ3lMbu/fv19dlxpurVq14ObmZni8adOmSE9Px4kTJ1TT+ZUrV9CqVav7lqFmzZqG6/JaHh4eiIyMVLcHDhyoavRhYWFo06YNunbtiiZNmjziX01knRioiayQBMbsTdH5RfqUc8PBwSHLbQnwEuyF9I+Hh4dj+fLlWLNmjQr60pQ+efLkAikzkSVjHzVREbRjx467blepUkVdl0vpu5a+as3WrVtha2uL0NBQuLu7Izg4GGvXrn2kMshAst69e+O3337D1KlT8eOPPz7S6xFZK9aoiaxQcnIyIiIistxnb29vGLAlA8Tq16+PZs2a4ffff8euXbvwf//3f+oxGfT1ySefqCA6ZswYXL9+HUOHDkWvXr3g7++v9pH733jjDZQoUULVjuPj41Uwl/1y4+OPP0a9evXUqHEp69KlSw0nCkSUFQM1kRVauXKlmjJlTGrDx48fN4zInjdvHgYNGqT2++OPP1C1alX1mEynWrVqFYYPH44GDRqo29KfPGXKFMNrSRBPSkrC119/jXfeeUedADz33HO5Lp+joyNGjRqF8+fPq6b05s2bq/IQ0d1sZERZDvcTkZWSvuKFCxeqAVxEZP7YR01ERGTGGKiJiIjMGPuoiYoY9nYRWRbWqImIiMwYAzUREZEZY6AmIiIyYwzUREREZoyBmoiIyIwxUBMREZkxBmoiIiIzxkBNRERkxhioiYiIYL7+H45qPjbiU/VSAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc83ded-5f80-4e1c-bf4d-ccb59999d995",
   "metadata": {},
   "source": [
    "- 从上面的结果可以看出，模型开始时生成不可理解的单词串，而到最后，它能够产生语法上或多或少正确的句子\n",
    "- 然而，基于训练和验证集损失，我们可以看到模型开始过拟合\n",
    "- 如果我们检查它在最后写的一些段落，我们会发现它们逐字包含在训练集中——它只是记忆了训练数据\n",
    "- 稍后，我们将介绍可以在一定程度上缓解这种记忆的解码策略\n",
    "- 注意这里的过拟合发生是因为我们有一个非常非常小的训练集，并且我们多次迭代它\n",
    "  - 这里的LLM训练主要用于教育目的；我们主要想看到模型可以学会产生连贯的文本\n",
    "  - 我们不是花费数周或数月在大量昂贵硬件上训练这个模型，而是稍后加载预训练权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb380c42-b31c-4ee1-b8b9-244094537272",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-2.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de713235-1561-467f-bf63-bf11ade383f0",
   "metadata": {},
   "source": [
    "**如果你有兴趣用更高级的技术来增强这个训练函数，如学习率预热、余弦退火和梯度裁剪，请参考[附录D](../../appendix-D/01_main-chapter-code)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cdf2f-09a5-4eb0-a20a-d7aac5c14c2c",
   "metadata": {},
   "source": [
    "**如果你对更大的训练数据集和更长的训练运行感兴趣，请参见[../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f45fc-bf78-42f2-bd24-2355db41b28f",
   "metadata": {
    "id": "699f45fc-bf78-42f2-bd24-2355db41b28f"
   },
   "source": [
    "## 5.3 控制随机性的解码策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9086e-2c27-41da-97d0-49137d0ba3c7",
   "metadata": {},
   "source": [
    "- 对于像我们上面训练的GPT模型这样相对较小的LLM，推理相对便宜，所以如果你在上面使用GPU进行训练，就不需要使用GPU进行推理\n",
    "- 使用我们之前在简单训练函数内部使用的`generate_text_simple`函数（来自前一章），我们可以一次生成一个单词（或token）的新文本\n",
    "- 如5.1.2节所解释的，下一个生成的token是对应于词汇表中所有token中最大概率分数的token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2734cee0-f6f9-42d5-b71c-fa7e0ef28b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25dbe31-bb7c-4893-b25b-47d0492d4aa4",
   "metadata": {},
   "source": [
    "- 即使我们多次执行上面的`generate_text_simple`函数，LLM也总是生成相同的输出\n",
    "- 我们现在介绍两个概念，即所谓的解码策略，来修改`generate_text_simple`：*温度缩放*和*top-k*采样\n",
    "- 这些将允许模型控制生成文本的随机性和多样性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6f380-a798-4fd9-825c-17b7cd29a994",
   "metadata": {},
   "source": [
    "### 5.3.1 温度缩放"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4f53c-0612-43d3-aa82-52447eac50fa",
   "metadata": {},
   "source": [
    "- 之前，我们总是使用`torch.argmax`采样具有最高概率的token作为下一个token\n",
    "- 为了增加多样性，我们可以使用`torch.multinomial(probs, num_samples=1)`来采样下一个token，从概率分布中采样\n",
    "- 在这里，每个索引被选中的机会对应于它在输入张量中的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7531bae-d5de-44c0-bc78-78fed077e22a",
   "metadata": {},
   "source": [
    "- 这里是生成下一个token的小回顾，假设一个非常小的词汇表用于说明目的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "01a5ce39-3dc8-4c35-96bc-6410a1e42412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# 假设输入是\"every effort moves you\"，LLM\n",
    "# 为下一个token返回以下logits：\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# 下一个生成的token如下：\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6400572f-b3c8-49e2-95bc-433e55c5b3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d0a27-830b-42b5-9986-6d1a7de04dd9",
   "metadata": {},
   "source": [
    "- 我们不是通过`torch.argmax`确定最可能的token，而是使用`torch.multinomial(probas, num_samples=1)`通过从softmax分布中采样来确定最可能的token\n",
    "- 为了说明目的，让我们看看当我们使用原始softmax概率采样下一个token 1,000次时会发生什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b23b863e-252a-403c-b5b1-62bc0a42319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n",
      "0 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # 为了可重现性设置手动种子\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample), minlength=len(probas))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7d9cf-a26d-4d9a-8664-4af1efa73832",
   "metadata": {},
   "source": [
    "- 我们可以通过一个叫做温度缩放的概念来控制分布和选择过程\n",
    "- \"温度缩放\"只是将logits除以一个大于0的数字的花哨说法\n",
    "- 大于1的温度在应用softmax后会导致更均匀分布的token概率\n",
    "- 小于1的温度在应用softmax后会导致更自信（更尖锐或更峰值）的分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b5399",
   "metadata": {},
   "source": [
    "- 注意结果的dropout输出可能因您的操作系统而异；您可以在[PyTorch问题跟踪器上](https://github.com/pytorch/pytorch/issues/121595)阅读更多关于这种不一致性的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0759e4c8-5362-467c-bec6-b0a19d1ba43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# 温度值\n",
    "temperatures = [1, 0.1, 5]  # 原始、更高置信度和更低置信度\n",
    "\n",
    "# 计算缩放概率\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750e989-842a-4cfa-a44b-cf44d6e49163",
   "metadata": {},
   "source": [
    "- 我们可以看到通过温度0.1的重新缩放导致了更尖锐的分布，接近`torch.argmax`，使得最可能的单词几乎总是被选中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4600713-c51e-4f53-bf58-040a6eb362b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n",
      "0 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e93cb-8e2a-42a1-b1ba-4fd5fe64c26b",
   "metadata": {},
   "source": [
    "- 通过温度5重新缩放的概率分布更加均匀："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9dfb48f0-bc3f-46a5-9844-33b6c9b0f4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 x closer\n",
      "75 x every\n",
      "42 x effort\n",
      "239 x forward\n",
      "71 x inches\n",
      "46 x moves\n",
      "32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83f0c4-3774-4375-ad7f-96440ba5fef7",
   "metadata": {},
   "source": [
    "- 假设LLM输入\"every effort moves you\"，使用上述方法有时会产生无意义的文本，如\"every effort moves you pizza\"，在3.2%的时间内（1000次中的32次）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8b8b8-8b8b-4b8b-8b8b-8b8b8b8b8b8b",
   "metadata": {},
   "source": [
    "### 5.3.2 Top-k采样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c9c9c9-c9c9-4c9c-c9c9-c9c9c9c9c9c9",
   "metadata": {},
   "source": [
    "- 另一种流行的方法是所谓的top-k采样，我们将概率质量限制在最可能的k个token上\n",
    "- 然后，我们重新分配概率质量，使这k个token的概率总和为1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4da95a-8bb2-4f69-a9b0-a643531db5df",
   "metadata": {},
   "source": [
    "- 为了能够使用更高的温度来增加输出多样性并减少无意义句子的概率，我们可以将采样的token限制在最可能的top-k个token上："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6fffd-2730-4abe-a2d3-781fc4836f17",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/topk.webp\" width=500px>\n",
    "\n",
    "- （请注意，此图中的数字被截断为小数点后两位以减少视觉混乱。Softmax行中的值应该加起来等于1.0。）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba12da5-6ff1-4008-91b8-d2d537cbc14c",
   "metadata": {},
   "source": [
    "- 在代码中，我们可以如下实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2a7f908a-e9ec-446a-b407-fb6dbf05c806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "753865ed-79c5-48b1-b9f2-ccb132ff1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa6fa49-6e99-459d-a517-d7d0f51c4f00",
   "metadata": {},
   "source": [
    "> 注意：  \n",
    ">\n",
    ">  前一个代码单元的另一种稍微更高效的实现如下：\n",
    ">\n",
    "> ```python\n",
    "> new_logits = torch.full_like( # 创建包含-inf值的张量\n",
    ">    next_token_logits, -torch.inf\n",
    ">)   \n",
    "> new_logits[top_pos] = next_token_logits[top_pos] # 将top k值复制到-inf张量中\n",
    "> ```\n",
    "> <br>\n",
    "> 更多详情，请参见 https://github.com/rasbt/LLMs-from-scratch/discussions/326\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4844f000-c329-4e7e-aa89-16a2c4ebee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56056503-a15d-4315-a3ff-46647a4c7c45",
   "metadata": {},
   "source": [
    "### 5.3.3 修改文本生成函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34770423-473d-46f6-a5fa-6b2979564d26",
   "metadata": {},
   "source": [
    "- 前面两个小节介绍了温度采样和top-k采样\n",
    "- 让我们使用这两个概念来修改我们之前用于通过LLM生成文本的`generate_simple`函数，创建一个新的`generate`函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8e318891-bcc0-4d71-b147-33ce55febfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For循环与之前相同：获取logits，只关注最后一个时间步\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # 新增：使用top_k采样过滤logits\n",
    "        if top_k is not None:\n",
    "            # 只保留top_k值\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # 新增：应用温度缩放\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # 应用softmax获得概率\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # 从分布中采样\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # 否则与之前相同：获取具有最高logits值的词汇表条目的idx\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # 如果遇到序列结束token且指定了eos_id，则提前停止生成\n",
    "            break\n",
    "\n",
    "        # 与之前相同：将采样的索引附加到运行序列中\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "aa2a0d7d-0457-42d1-ab9d-bd67683e7ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you stand to work on surprise, a one of us had gone with random-\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2002ca-f4c1-48af-9e0a-88bfc163ba0b",
   "metadata": {},
   "source": [
    "## 5.4 在PyTorch中加载和保存模型权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc52676-f026-4566-a226-2a90269f9d53",
   "metadata": {},
   "source": [
    "- 训练LLM在计算上是昂贵的，因此能够保存和加载LLM权重至关重要\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-3.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4c7f9-592f-43d6-a00e-598fa01dfb82",
   "metadata": {},
   "source": [
    "- PyTorch中推荐的方法是通过对`.state_dict()`方法应用`torch.save`函数来保存模型权重，即所谓的`state_dict`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3d67d869-ac04-4382-bcfb-c96d1ca80d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e889e0-07bf-43e5-8f92-5c5c7aeaad9e",
   "metadata": {},
   "source": [
    "- 然后我们可以将模型权重加载到新的`GPTModel`模型实例中，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9d57d914-60a3-47f1-b499-5352f4c457cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa81aec-9c72-4f46-8ae2-4a4fde3edbc1",
   "metadata": {},
   "source": [
    "- 通常使用自适应优化器如Adam或AdamW而不是常规SGD来训练LLM\n",
    "- 这些自适应优化器为每个模型权重存储额外的参数，因此如果我们计划稍后继续预训练，保存它们也是有意义的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "bbd175bb-edf4-450e-a6de-d3e8913c6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8a0c7295-c822-43bf-9286-c45abc542868",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194350e-0409-4a63-8ffd-d3a896509032",
   "metadata": {},
   "source": [
    "## 5.5 从OpenAI加载预训练权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb6c38-7278-40e0-bd9f-8a2b1feac3ec",
   "metadata": {},
   "source": [
    "- 之前，我们只是为了教育目的使用一本非常小的短篇小说书训练了一个小型GPT-2模型\n",
    "- 感兴趣的读者也可以在[../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)中找到在完整的古腾堡计划书籍语料库上的更长预训练运行\n",
    "- 幸运的是，我们不必花费数万到数十万美元在大型预训练语料库上预训练模型，而是可以加载OpenAI提供的预训练权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ddbdb-3878-4669-9a39-d231fbdfb834",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "⚠️ **注意：由于TensorFlow兼容性问题，特别是在某些Windows系统上，一些用户可能在本节中遇到问题。这里只需要TensorFlow来加载原始的OpenAI GPT-2权重文件，然后我们将其转换为PyTorch。\n",
    "如果您遇到TensorFlow相关问题，可以使用下面的替代代码而不是本节中的其余代码。\n",
    "这个替代方案基于预转换的PyTorch权重，使用与前一节中描述的相同转换过程创建。详情请参考notebook：\n",
    "[../02_alternative_weight_loading/weight-loading-pytorch.ipynb](../02_alternative_weight_loading/weight-loading-pytorch.ipynb) notebook。**\n",
    "\n",
    "```python\n",
    "file_name = \"gpt2-small-124M.pth\"\n",
    "# file_name = \"gpt2-medium-355M.pth\"\n",
    "# file_name = \"gpt2-large-774M.pth\"\n",
    "# file_name = \"gpt2-xl-1558M.pth\"\n",
    "\n",
    "url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    print(f\"Downloaded to {file_name}\")\n",
    "\n",
    "gpt = GPTModel(BASE_CONFIG)\n",
    "gpt.load_state_dict(torch.load(file_name, weights_only=True))\n",
    "gpt.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt.to(device);\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cab892-a165-4f43-9601-f517bc212ab6",
   "metadata": {},
   "source": [
    "- 首先，一些样板代码来从OpenAI下载文件并将权重加载到Python中\n",
    "- 由于OpenAI使用了[TensorFlow](https://www.tensorflow.org/)，我们将必须安装和使用TensorFlow来加载权重；[tqdm](https://github.com/tqdm/tqdm)是一个进度条库\n",
    "- 取消注释并运行下一个单元格来安装所需的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb9fdf02-972a-444e-bf65-8ffcaaf30ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a0747edc-559c-44ef-a93f-079d60227e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c5bc89eb-4d39-4287-9b0c-e459ebe7f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从此文件夹中包含的gpt_download.py进行相对导入\n",
    "\n",
    "from gpt_download import download_and_load_gpt2\n",
    "# 或者：\n",
    "# from llms_from_scratch.ch05 import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76a736-6f9f-4328-872e-f89a7b70a2cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**注意**\n",
    "\n",
    "- 在极少数情况下，上面的代码单元格可能会导致`zsh: illegal hardware instruction python`错误，这可能是由于您机器上的TensorFlow安装问题\n",
    "- 一位读者发现通过`conda`安装TensorFlow解决了这个特定情况下的问题，如[这里](https://github.com/rasbt/LLMs-from-scratch/discussions/273#discussioncomment-12367888)所述\n",
    "- 您可以在这个补充的[Python设置教程](https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/01_optional-python-setup-preferences#option-2-using-conda)中找到更多说明\n",
    "\n",
    "---\n",
    "\n",
    "- 然后我们可以如下下载1.24亿参数模型的模型权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "76271dd7-108d-4f5b-9c01-6ae0aac4b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 76.9kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 824kiB/s] \n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 90.1kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [01:11<00:00, 6.96MiB/s]   \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 12.3MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 526kiB/s]  \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 497kiB/s] \n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b1a31951-d971-4a6e-9c43-11ee1168ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "857c8331-130e-46ba-921d-fa35d7a73cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c48dac94-8562-4a66-84ef-46c613cdc4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e100c-294e-4afc-a70a-2f398ac4c104",
   "metadata": {},
   "source": [
    "- 或者，\"355M\"、\"774M\"和\"1558M\"也是支持的`model_size`参数\n",
    "- 这些不同大小模型之间的差异在下图中总结："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f19d32-5aae-4176-9f86-f391672c8f0d",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-sizes.webp?timestamp=123\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e5076-f08d-41fc-bd8b-1cfe53538f41",
   "metadata": {},
   "source": [
    "- 上面，我们将124M GPT-2模型权重加载到Python中，但是我们仍然需要将它们传输到我们的`GPTModel`实例中\n",
    "- 首先，我们初始化一个新的GPTModel实例\n",
    "- 注意原始GPT模型在多头注意力模块中为查询、键和值矩阵的线性层初始化了偏置向量，这不是必需的或推荐的；但是，为了能够正确加载权重，我们也必须通过在我们的实现中将`qkv_bias`设置为`True`来启用这些\n",
    "- 我们还使用了原始GPT-2模型使用的`1024`token上下文长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9fef90dd-0654-4667-844f-08e28339ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在字典中定义模型配置以保持紧凑\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# 复制基础配置并使用特定模型设置更新\n",
    "model_name = \"gpt2-small (124M)\"  # 示例模型名称\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f29ac-8342-4b3d-a57d-9b0166ced314",
   "metadata": {},
   "source": [
    "- 下一个任务是将OpenAI权重分配给我们`GPTModel`实例中相应的权重张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9a92229-c002-49a6-8cfb-248297ad8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f22d5d95-ca5a-425c-a9ec-fc432a12d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7472cb-54dc-4311-96d8-b2694f885cee",
   "metadata": {},
   "source": [
    "- 如果模型加载正确，我们可以使用之前的`generate`函数来生成新文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f690253-f845-4347-b7b6-43fabbd2affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward finding an ideal new way to practice something!\n",
      "\n",
      "What makes us want to be on top of that?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d079f98-a7c4-462e-8416-5a64f670861c",
   "metadata": {},
   "source": [
    "- 我们知道我们正确加载了模型权重，因为模型可以生成连贯的文本；如果我们犯了哪怕一个小错误，模型都无法做到这一点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28493b9b-a1ae-4f31-87bc-c10ee4447f44",
   "metadata": {},
   "source": [
    "- 有关从Hugging Face Hub加载权重的替代方法，请参见[../02_alternative_weight_loading](../02_alternative_weight_loading)\n",
    "- 如果您有兴趣了解GPT架构与Llama架构（Meta AI开发的流行LLM）的比较，请参见[../07_gpt_to_llama](../07_gpt_to_llama)的奖励内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a66474-230d-4180-a8ff-843e04f1f1c4",
   "metadata": {},
   "source": [
    "## 总结和要点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ed189-a633-458c-bf12-4f70b42684b8",
   "metadata": {},
   "source": [
    "- 请参见[./gpt_train.py](./gpt_train.py)脚本，这是一个用于训练的独立脚本\n",
    "- [./gpt_generate.py](./gpt_generate.py)脚本从OpenAI加载预训练权重并基于提示生成文本\n",
    "- 您可以在[./exercise-solutions.ipynb](./exercise-solutions.ipynb)中找到练习解答"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygtml_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
