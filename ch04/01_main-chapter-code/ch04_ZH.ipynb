{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f4321d-d32a-4a90-bfc7-e923f316b2f8",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "<a href=\"http://mng.bz/orYv\">从零开始构建大型语言模型</a> 一书的补充代码，作者：<a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>代码仓库：<a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9295b2-182b-490b-8325-83a67c4a001d",
   "metadata": {},
   "source": [
    "# 第4章：从零开始实现GPT模型以生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9eac223-a125-40f7-bacc-bd0d890450c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.5\n",
      "torch version: 2.7.1\n",
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da97ed-e02f-4d7f-b68e-a0eba3716e02",
   "metadata": {},
   "source": [
    "- 在本章中，我们实现一个类似GPT的LLM架构；下一章将专注于训练这个LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f11e0-4434-4979-9dee-e1207df0eb01",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/01.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe99ab-0bcf-4778-a6b5-6db81fb826ef",
   "metadata": {},
   "source": [
    "## 4.1 编码LLM架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72d1ff-d82d-4e33-a88e-3c1a8831797b",
   "metadata": {},
   "source": [
    "- 第1章讨论了像GPT和Llama这样的模型，它们按顺序生成单词，基于原始transformer架构的解码器部分\n",
    "- 因此，这些LLM通常被称为\\\"类解码器\\\"LLM\n",
    "- 与传统的深度学习模型相比，LLM更大，主要是由于它们的大量参数，而不是代码量\n",
    "- 我们将看到LLM架构中有许多重复的元素"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5213e9-bd1c-437e-aee8-f5e8fb717251",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/02.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43f5e2-fb51-434a-b9be-abeef6b98d99",
   "metadata": {},
   "source": [
    "- 在前面的章节中，我们为了便于说明使用了较小的token输入和输出嵌入维度，确保它们能在单页上显示\n",
    "- 在本章中，我们考虑类似于小型GPT-2模型的嵌入和模型大小\n",
    "- 我们将专门编码最小的GPT-2模型（1.24亿参数）的架构，如Radford等人的[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)中所述（注意初始报告将其列为117M参数，但这在模型权重仓库中后来得到了纠正）\n",
    "- 第6章将展示如何将预训练权重加载到我们的实现中，这将与3.45亿、7.62亿和15.42亿参数的模型大小兼容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21baa14d-24b8-4820-8191-a2808f7fbabc",
   "metadata": {},
   "source": [
    "- 1.24亿参数GPT-2模型的配置详情包括："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ed66875-1f24-445d-add6-006aae3c5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # 词汇表大小\n",
    "    \"context_length\": 1024, # 上下文长度\n",
    "    \"emb_dim\": 768,         # 嵌入维度\n",
    "    \"n_heads\": 12,          # 注意力头数量\n",
    "    \"n_layers\": 12,         # 层数\n",
    "    \"drop_rate\": 0.1,       # Dropout率\n",
    "    \"qkv_bias\": False       # 查询-键-值偏置\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fcd28-d210-4c57-8be6-06cfcd5d73a4",
   "metadata": {},
   "source": [
    "- 我们使用简短的变量名以避免后面代码行过长\n",
    "- `\"vocab_size\"` 表示词汇表大小为50,257个单词，由第2章讨论的BPE分词器支持\n",
    "- `\"context_length\"` 表示模型的最大输入token数量，由第2章涵盖的位置嵌入启用\n",
    "- `\"emb_dim\"` 是token输入的嵌入大小，将每个输入token转换为768维向量\n",
    "- `\"n_heads\"` 是第3章实现的多头注意力机制中的注意力头数量\n",
    "- `\"n_layers\"` 是模型内transformer块的数量，我们将在接下来的部分中实现\n",
    "- `\"drop_rate\"` 是dropout机制的强度，在第3章中讨论；0.1意味着在训练期间丢弃10%的隐藏单元以减轻过拟合\n",
    "- `\"qkv_bias\"` 决定多头注意力机制中的 `Linear` 层（来自第3章）在计算查询(Q)、键(K)和值(V)张量时是否应包含偏置向量；我们将禁用此选项，这是现代LLM的标准做法；但是，当我们在第5章将OpenAI的预训练GPT-2权重加载到我们的重新实现中时，我们将重新讨论这一点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adce779-857b-4418-9501-12a7f3818d88",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/03.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "619c2eed-f8ea-4ff5-92c3-feda0f29b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # 使用TransformerBlock的占位符\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # 使用LayerNorm的占位符\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # 一个简单的占位符\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 一个简单的占位符\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # 一个简单的占位符\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 一个简单的占位符\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fcd28-d210-4c57-8be6-06cfcd5d73a4",
   "metadata": {},
   "source": [
    "- 上面的代码定义了一个占位符GPT模型架构，其中包含token和位置嵌入层、dropout、transformer块、层归一化和线性输出层\n",
    "- 我们将在接下来的部分中用实际实现替换占位符类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ed66875-1f24-445d-add6-006aae3c5707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 4])\n",
      "输出形状: torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(\"输入形状:\", batch.shape)\n",
    "\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"输出形状:\", logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fcd28-d210-4c57-8be6-06cfcd5d73a4",
   "metadata": {},
   "source": [
    "- 输出张量的形状是 `[batch_size, num_tokens, vocab_size]`\n",
    "- 这意味着对于批次中的每个输入token，模型输出一个50,257维的向量（对应于词汇表中的每个token）\n",
    "- 接下来，我们将实现各个组件以创建一个功能完整的GPT模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fad0fe-895d-4493-9e48-962e2d46c66f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**注意**\n",
    "\n",
    "- 如果您在Windows或Linux上运行此代码，上面的结果值可能如下所示：\n",
    "    \n",
    "```\n",
    "Output shape: torch.Size([2, 4, 50257])\n",
    "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
    "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
    "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
    "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
    "\n",
    "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
    "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
    "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
    "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
    "       grad_fn=<UnsafeViewBackward0>)\n",
    "```\n",
    "\n",
    "- 由于这些只是随机数，这不是令人担心的原因，您可以继续本章的其余部分而不会有问题\n",
    "- 造成这种差异的一个可能原因是 `nn.Dropout` 在不同操作系统上的行为不同，这取决于PyTorch的编译方式，如[PyTorch问题跟踪器](https://github.com/pytorch/pytorch/issues/121595)中所讨论的\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8332a00-98da-4eb4-b882-922776a89917",
   "metadata": {},
   "source": [
    "## 4.2 使用层归一化规范化激活"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cfb81-d59b-4d95-afe3-e43cf095f292",
   "metadata": {},
   "source": [
    "- 层归一化，也称为LayerNorm（[Ba等人，2016](https://arxiv.org/abs/1607.06450)），将神经网络层的激活以0为中心，并将其方差归一化为1\n",
    "- 这稳定了训练并使其能够更快地收敛到有效权重\n",
    "- 层归一化在transformer块内的多头注意力模块之前和之后都会应用，我们稍后会实现；它也在最终输出层之前应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ac47a-69cc-4597-beeb-65bed3b5910f",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/05.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab49940-6b35-4397-a80e-df8d092770a7",
   "metadata": {},
   "source": [
    "- 让我们实现层归一化，从一个简单的例子开始："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "009238cd-0160-4834-979c-309710986bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# 创建2个样本，每个有5个维度（特征）\n",
    "batch_example = torch.randn(2, 5) \n",
    "print(batch_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "009238cd-0160-4834-979c-309710986bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n"
     ]
    }
   ],
   "source": [
    "print(batch_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab49940-6b35-4397-a80e-df8d092770a7",
   "metadata": {},
   "source": [
    "- 层归一化通常在最后一个维度上应用，这对应于嵌入维度\n",
    "- 这意味着我们计算每个样本（行）的均值和方差："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "009238cd-0160-4834-979c-309710986bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均值: tensor([[-0.3596],\n",
      "        [-0.2606]])\n",
      "方差: tensor([[0.2518],\n",
      "        [0.3342]])\n"
     ]
    }
   ],
   "source": [
    "layer = batch_example[0]  # 第一个样本\n",
    "\n",
    "mean = batch_example.mean(dim=-1, keepdim=True)\n",
    "var = batch_example.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"均值:\", mean)\n",
    "print(\"方差:\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab49940-6b35-4397-a80e-df8d092770a7",
   "metadata": {},
   "source": [
    "- 现在让我们应用层归一化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "009238cd-0160-4834-979c-309710986bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "归一化后:\n",
      " tensor([[ 0.4945,  0.9564, -0.0200,  0.2375, -1.6685],\n",
      "        [ 0.8127, -1.2313, -0.8554,  1.0110,  0.2630]])\n"
     ]
    }
   ],
   "source": [
    "out_norm = (batch_example - mean) / torch.sqrt(var + 1e-5)\n",
    "print(\"归一化后:\\n\", out_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab49940-6b35-4397-a80e-df8d092770a7",
   "metadata": {},
   "source": [
    "- 让我们验证归一化后的输出确实具有0均值和1方差："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "009238cd-0160-4834-979c-309710986bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "归一化后的均值: tensor([-1.4901e-08,  2.3842e-08])\n",
      "归一化后的方差: tensor([1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"归一化后的均值:\", mean.flatten())\n",
    "print(\"归一化后的方差:\", var.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fccc29e-71fc-4c16-898c-6137c6ea5d2e",
   "metadata": {},
   "source": [
    "- 现在让我们实现一个LayerNorm类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9888f79e-8e69-44aa-8a19-cd34292adbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eda3e-b395-48c4-acd4-eb8083bab958",
   "metadata": {},
   "source": [
    "- `scale` 和 `shift` 是可学习的参数，允许模型学习适当的缩放和偏移\n",
    "- 归一化应用于每个输入（行）独立地；使用dim=-1在最后一个维度（在这种情况下是特征维度）而不是行维度上应用计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a1d1bb9-3341-4c9a-bc2a-d2489bf89cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "print(out_ln)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570db83a-205c-4f6f-b219-1f6195dde1a7",
   "metadata": {},
   "source": [
    "- 让我们验证我们的LayerNorm实现与PyTorch的内置实现产生相同的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f8ecbc7-eb14-4fa1-b5d0-7e1ff9694f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们的实现: tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n",
      "PyTorch实现: tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "\n",
    "# PyTorch的内置LayerNorm\n",
    "ln_pytorch = nn.LayerNorm(5)\n",
    "out_ln_pytorch = ln_pytorch(batch_example)\n",
    "\n",
    "print(\"我们的实现:\", out_ln)\n",
    "print(\"PyTorch实现:\", out_ln_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fb958-d4ed-43cc-858d-00052bb6b31a",
   "metadata": {},
   "source": [
    "**缩放和偏移**\n",
    "\n",
    "- 注意，除了通过减去均值和除以方差来执行归一化之外，我们还添加了两个可训练参数，一个 `scale` 和一个 `shift` 参数\n",
    "- 初始的 `scale`（乘以1）和 `shift`（加0）值没有任何效果；但是，`scale` 和 `shift` 是可训练参数，如果确定这样做会提高模型在训练任务上的性能，LLM会在训练期间自动调整它们\n",
    "- 这允许模型学习最适合其处理数据的适当缩放和偏移\n",
    "- 注意我们还在计算方差的平方根之前添加了一个较小的值（`eps`）；这是为了避免在方差为0时出现除零错误\n",
    "\n",
    "**有偏方差**\n",
    "- 在上面的方差计算中，设置 `unbiased=False` 意味着使用公式 $\\frac{\\sum_i (x_i - \\bar{x})^2}{n}$ 来计算方差，其中n是样本大小（这里是特征或列的数量）；这个公式不包括贝塞尔校正（在分母中使用 `n-1`），因此提供了方差的有偏估计\n",
    "- 对于LLM，其中嵌入维度 `n` 非常大，使用n和 `n-1` 之间的差异是可以忽略的\n",
    "- 但是，GPT-2在归一化层中使用有偏方差进行训练，这就是为什么我们也采用这种设置以与我们将在后续章节中加载的预训练权重兼容的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136cfc4-7c89-492e-b120-758c272bca8c",
   "metadata": {},
   "source": [
    "## 4.3 使用GELU激活函数实现前馈网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136cfc4-7c89-492e-b120-758c272bca8c",
   "metadata": {},
   "source": [
    "- 在本节中，我们实现一个小的神经网络子模块，用作transformer块中的前馈网络\n",
    "- 这个前馈网络由两个 `Linear` 层和一个GELU激活函数组成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136cfc4-7c89-492e-b120-758c272bca8c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/07.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136cfc4-7c89-492e-b120-758c272bca8c",
   "metadata": {},
   "source": [
    "- GELU（高斯误差线性单元）激活函数是ReLU函数的平滑版本\n",
    "- 与ReLU不同，ReLU在零处有一个尖锐的角，GELU是一个平滑的、可微分的曲线\n",
    "- 让我们比较GELU和ReLU激活函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e136cfc4-7c89-492e-b120-758c272bca8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cruld\\AppData\\Local\\Temp\\ipykernel_8704\\3488672341.py:19: UserWarning: Glyph 28608 (\\N{CJK UNIFIED IDEOGRAPH-6FC0}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\cruld\\AppData\\Local\\Temp\\ipykernel_8704\\3488672341.py:19: UserWarning: Glyph 27963 (\\N{CJK UNIFIED IDEOGRAPH-6D3B}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\cruld\\AppData\\Local\\Temp\\ipykernel_8704\\3488672341.py:19: UserWarning: Glyph 20989 (\\N{CJK UNIFIED IDEOGRAPH-51FD}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\cruld\\AppData\\Local\\Temp\\ipykernel_8704\\3488672341.py:19: UserWarning: Glyph 25968 (\\N{CJK UNIFIED IDEOGRAPH-6570}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAEiCAYAAABkykQ1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUotJREFUeJzt3Qd4VMXaB/B/eoMEQkkooUMglEASqSrgR8eCBbkoTQEbKIgXBS6iyFVURFDpFlAEqQJeQASRIgJCCoQWlBJCSSWkkJ7d/Z6ZsJGEUDab5JT9/57nPNk9ORtmWNjJe2bed+xMJpMJREREREREVrC35sVEREREREQCAwsiIiIiIrIaAwsiIiIiIrIaAwsiIiIiIrIaAwsiIiIiIrIaAwsiIiIiIrIaAwsiIiIiIrIaAwsiIiIiIrIaAwsiIiIiIrIaAwsiIiIiIrIaAwvSrfPnz2Ps2LFo1qwZ3N3d5REQEIAxY8YgMjKyyLXvvvsu7OzsbnvExcXJ66Kjo+XzTz755LZ/boMGDfDwww+X+L3Q0FD5+mXLlpVxb4mI6F6Iz9+bP98dHR1Rp04djBgxApcvXy7Vz9y9e7f8WevWrbvtNeL7YkwqiXid+L74OURa5qh0A4jKw+bNmzFo0CA5YDz77LMIDAyEvb09oqKi8OOPP2LhwoUy8Khfv36R14nzlSpVuuXnValSpcLanpGRIf88FxeXEr+fl5eHn3/+GR06dFDkuoceesiK3hERqcN7772Hhg0bIjs7GwcPHpQBx759+3D8+HG4urpCbU6cOIF27drB2dm5xO/n5ubi1KlTsj9KXNe4cWMrekd6wcCCdOfs2bP417/+JYOGnTt3olatWkW+/9FHH2HBggUy0CjuqaeeQvXq1aEkk8kEHx8fXLp0qcTvi74ZjUbFriMi0oO+ffsiJCREPh41apT87Bfjw08//YSnn34aaiM+o9u3by+Dn5J07NhRXqPUdUQCl0KR7nz88cfyrv/SpUtvCSoEMYvx2muvwc/PT5H2ERGR+jzwwAOFN6duJma6xU0nb29vOZMhghERfBDRrThjQbpcBtWkSRO5tMdSycnJJQYiFbkUioiIKp7IoROqVq1aZPlRly5dZA7GpEmT4OHhgTVr1mDAgAFYv349Hn/8cQVbTKQ+DCxIV9LS0nDlyhX5oV9cSkoK8vPzC5+LAcLNza3INf7+/re8TpwTd6yIiEg/UlNTkZSUJHMI/vzzT0yfPl3mmN1cfGPcuHGoV68eDh8+XJh/9sorr+D+++/HW2+9xcCCqBgGFqS7wEIoKQG7W7duOHr0aOHzWbNm4d///neRa8QdKE9PzyLnRABCRET60qNHj1sq+n3//feoW7du4Qz2b7/9JpO809PT5WHWu3dvvPPOO7KKlJjNIKICDCxIVypXriy/Xr9+/ZbvLV68WA4M8fHxGDJkSImvf/DBByskeVuUFSQiIuXMnz9fliMXMxfffPMN9u7dW6Qq3pkzZ2RS8ttvvy2PkiQkJJRpYMGxgbSOgQXpipeXl0zYFuUCizPnXJjX0ZYXkdyXlZVV4vcyMzMLryEiIuWISkfmqlBi+axY3vTMM8/g9OnTctbbXAVPzGyLGYqSiHy+eyWCFo4NpHcMLEh3+vfvj6+++gqHDh2SA0dFE2VuT548WeL3xIBlvoaIiNTBwcEBM2fORPfu3TFv3jyZqN2oUSP5PScnp1uWTZWG+Nw3jwHFcWwgvWC5WdKdN998U+6y/fzzz8tlT8WVd73tfv36yb0gNm7cWOR8Tk6ODHhq1qyJoKCgcm0DERFZRuThiZtRc+fOlQnd4rNanBPLaGNjY2+5PjEx0eKxQWzEFxYWdkthkRUrVqBt27bw9fW1uh9ESuKMBelO06ZNsXLlSgwePFhWdDLvvC0CCrHbtvie2BzPnKB3s3Xr1pWY+N2zZ0+5eZyZ2HhPDDzFien0F154Qa7XHThwoAxuxI6lV69exerVq+USre++++62O5gSEZFyJk6cKD+7xS7cL730kszDEEukWrdujdGjR8tZDHHD6sCBA/IG0s0FQcwFQEqqIjh8+HA5C7J27VqZy/fiiy+iefPmsoqh+LNE4CL2XiLSOgYWpEuPPfYYjh07htmzZ2P79u3yF32RFCemmcVSKTFgiGCjuJdffrnEn7dr164igcW2bdvkUZyoKtKqVSvs2bNHVhIRsxZisBBlbYODg7F161b06dOnjHtLRERl4YknnkDjxo3xySefyEAiICAAoaGhshStCADETSIxkyFuGE2bNu2W169atarEnytmPkSAIsravvvuu3IvDBGgiCqEnTt3ljeeSrP3EpHaMLAg3RKDw4IFC+7pWvFBL467EYHDvSylEhvqffrpp/IgIiL1GDFihDxKImazRTWom4lZim+//faOP1MEDvcyNogKUl9++aWFLSbSDuZYEBERERGR1ThjQaRCYt2tmPW4XVnCUaNGKXodERFVPJH8fbvP6Jv3b1LqOiI7U3mXyCEiIiIiIt3jUigiIiIiIrIaAwsiIiIiIrIaAwsiIiIiIrKazSVvG41GmaBauXJlua8BEREV7Eifnp6O2rVry5KbtopjBBFR6ccHmwssxIDh5+endDOIiFTp4sWLJe5Kbys4RhARlX58sLnAQtyFMv/liB0vLZGXlyd3ce7VqxecnJygVXroB/ugHnrohx76YG0/0tLS5C/U5s9IW2XrYwT7oB566Ice+qCXfuRV0Phgc4GFeWpbDBilGTTc3d3l67T6D0sv/WAf1EMP/dBDH8qqH7a+/MfWxwj2QT300A899EEv/ciroPHBdhfSEhERERFRmWFgQURERERE2g4sFi5ciDZt2hROOXfq1Ak///zzHV+zdu1aNG/eHK6urmjdujW2bt1aYe0lIqKKwfGBiEh7FA0sRGb5hx9+iLCwMISGhuKhhx7CY489hhMnTpR4/f79+zF48GCMHDkSERERGDBggDyOHz9e4W0nIqLyw/GBiEh7FA0sHnnkEfTr1w9NmzZFs2bN8P7776NSpUo4ePBgidd/9tln6NOnDyZOnIgWLVpgxowZCAoKwrx58yq87UREVH44PhARaY9qqkIZDAY5jZ2RkSGnvEty4MABTJgwoci53r17Y+PGjbf9uTk5OfK4uWSWOTteHJYwX2/p69RGD/1gH9RDD/3QRR8MRry3+SSaGUrXDzX3vbzGByIiW/H730n47Yod+ppM+g4sjh07JgeK7OxseTdqw4YNCAgIKPHauLg4+Pj4FDknnovztzNz5kxMnz79lvOilq8ou1UaO3bsgB7ooR/sg3rooR9a7sOac/b4I94e1VwcUMV5BxwsnI/OzMyE2pT3+CDw5lNR7IN66KEfeuiDHvpxITkT49dEIi3bASGHY/Cv9vUter0l/VY8sPD398eRI0eQmpqKdevWYfjw4dizZ89tBw9LTZ48uchdLPMmH2KDkNLUKBe/ePTs2VOzdYz10g/2QT300A+t92HFnzH440AURIXxxxsY0ae35f0w/0KtJuU9Pgi8+VQy9kE99NAPPfRBq/3IMQBzjjsgLdsO9SuZ4J5wAlu3lpyrVhY3nhQPLJydndGkSRP5ODg4GIcPH5ZrZRcvXnzLtb6+voiPjy9yTjwX52/HxcVFHsWJQbe0v0BY81o10UM/2Af10EM/tNiHfX8nYcbW0/LxGz2bwu/6qVL1Q439Lu/xQeDNp6LYB/XQQz/00Act98NkMsmZitjMeFTzcMbzzTLRt5xvPCkeWBRnNBqLTEvfTEyJ79y5E+PHjy88J97o2625JSLSs/NJGXhlRRgMRhOeCKqDFx5ogJ9/PgW9Ko/xgTefSsY+qIce+qGHPmixH4v2nMXW4/FwtLfDvMGBSDhxoNxvPCkaWIg7RX379kW9evWQnp6OlStXYvfu3fjll1/k94cNG4Y6derIqWph3Lhx6Nq1K2bPno3+/ftj1apVsgzhkiVLlOwGEVGFS83Kw8hvDyMtOx9B9argg8dbww5G6AXHByKi0tv7VyI+3hYlH7/zaEuE1K8KC1dAlYqigUVCQoIcHGJjY+Hl5SU3QxKDhphqEmJiYmBv/08GYufOneXgMnXqVEyZMkWWIRQVP1q1aqVgL4iIKla+wYixK8NxLjEDtb1csXhoCFydHJCXp5/AguMDEVHpxFzNxKs/RMBoAgYG18WQDvWQn5+PiqBoYPH111/f8fvi7lRxAwcOlAcRka3675ZTsnSgm5MDvhweghqVb13Ko3UcH4iILJeZm48XlofKWe1AvyqYMaAV7OxEaQ8b2CCPiIgs88OhGCzbHy0fzxkUiJa1vZRuEhERqSRZ+631xxAVl47qlZyxaEiQnM2uSAwsiIg04uC5q3h743H5+I2ezdCnVS2lm0RERCrx1e/n8b+jV2Sy9oJng1HLy63C28DAgohII2tmX/4+DPlGEx4JrI2xDxWUYSUiItr3dxJm3qgK+PbDAWjf0FuRdjCwICJSues5+Rj13WFcy8xDm7pemPVUmwpdM0tEROp1MTkTY38Il8naTwXXxbBOlu2sXZYYWBARqZjYo2LcDxH4K/46alZ2wZIbFaCIiIiycg14cXkYUm7cePpvBSdrF8fAgohIxWb9cho7oxLg7GiPJcNC4OvlqnSTiIhIJcnak36MxMnYNLmz9qIhwYrfeGJgQUSkUhsiLsmdUwWx/KmtXxWlm0RERCrx9b7z2HTkChzs7TD/2SDUrlLxydrFMbAgIlKhiJhrsmygMKZ7YzzWto7STSIiIpXYf0YkaxfsrD21fwt0bFQNasDAgohIZeJSs+Wa2dx8I3oG+OCNnv5KN4mIiFTi0jWRrB0hc/CeCKqDEZ0bQC0YWBARqUh2nkHumpqQngN/n8qYM6gt7O1ZAYqIiCDHCHHjKTkjF63qeOKDx1urqkogAwsiIhUl4r25LhKRl1JR1d0JXw0PQSUXR6WbRUREKhkjpvx4DCeupMFbJcnaxTGwICJSiQW7z+Knm3ZN9fN2V7pJRESkEsv2R+PHiMsyWXveM+1Qt6r6xggGFkREKrDjZDw+2X5aPn730Zbo1FgdiXhERKS8g+eu4r9bCnbWntKvBTo3rg41YmBBRKSwv+LTMX5VBEwmYEjHehjSUbldU4mISF0up2RhzIpwmaw9oG1tPN9FPcnaxTGwICJS0LWMXIz6NhQZuQZ0bOSNdx5pqXSTiIhIRcnaL38fhqsZuQio5YmZT7RRVbJ2cQwsiIgUkmcwYszKcMQkZ8LP203mVTg58GOZiIggk7X/s+F4YUGPxUOD4easrmTt4jiCEREp5P0tp7D/7FW4Ozvgy2EhssoHERGR8N2BC1gffgmi4vi8Z4I0UdCDgQURkQJWH46RFT4EsVdFc19PpZtEREQq8ee5q5ix+aR8PLlvC3Rpos5kbVUFFjNnzsR9992HypUro2bNmhgwYABOny6oinI7y5Ytk2vLbj5cXV0rrM1ERNYKjU7G1I3H5eMJPZuhd0tfpZtEREQqEZuaJZfJ5htNeDSwNkY90BBaoWhgsWfPHowZMwYHDx7Ejh07kJeXh169eiEjI+OOr/P09ERsbGzhceHChQprMxGRNa6kZOGl78OQZzChX2tfvPpQE6WbREREKkrWfml5GJKu56JFLU989KS6k7VVFVhs27YNI0aMQMuWLREYGChnI2JiYhAWFnbH14m/YF9f38LDx8enwtpMRFRaWbkGvLA8tHDA+GRgoKYGjIrEGW0issVk7bc3HsfRS6nwcnPC4iHqT9ZWdY5Famqq/Ort7X3H665fv4769evDz88Pjz32GE6cOFFBLSQiKv2AMenHSBy/nCaTtJcMDYa7s6PSzVItzmgTka35/s8YrA0zJ2u3Q71q6k/WLk41o5rRaMT48ePRpUsXtGrV6rbX+fv745tvvkGbNm1kIPLJJ5+gc+fOMrioW7fuLdfn5OTIwywtLU1+FYOUOCxhvt7S16mNHvrBPqiHHvpREX34ct95bDpyBY72dvh8UBv4VnYq8z/Pmn6o7f0TM9rFZyPEzIWY0X7wwQfvOqNNRKQlh6OTMf2nghvlb/Vpjgea1oAWqSawEHemjh8/jn379t3xuk6dOsnDTAQVLVq0wOLFizFjxowSp9OnT59+y/nt27fD3b10kaC4e6YHeugH+6AeeuhHefXh1DU7LI4SE8R2GFA/H1dPHcTWU1BVPzIzM6Fmls5oi5tVQUFB+OCDD+RyWyIitYpPy8YrKwqStfu3qYUXHmwErVJFYDF27Fhs3rwZe/fuLXHW4U6cnJzQrl07nDlzpsTvT548GRMmTCgyYyGWUIkpdTFlbukdPTFg9+zZU/65WqWHfrAP6qGHfpRnH84nZWDq4j9hQj4GhdTBjEcDyi2vwpp+mGdz1ai8ZrQFzmoXxT6ohx76oYc+lHc/cvKNeHF5KBLTc+DvUwnvP9oC+fn5Zf7nVNSMtqPSa45fffVVbNiwAbt370bDhpaX0zIYDDh27Bj69etX4vddXFzkUZwYdEv7C4Q1r1UTPfSDfVAPPfSjrPuQnp2HV344ivTsfATXr4oZA9rA2dFelf1Q83tXXjPaAme1S8Y+qIce+qGHPpRXP1adtceRBHu4O5jwdO0U7Nm5HeWpvGe0HZUeLFauXIlNmzbJyh9xcXHyvJeXF9zc3OTjYcOGoU6dOvLDX3jvvffQsWNHNGnSBCkpKZg1a5ZMzhs1apSSXSEiKsJoNOH11UdwJuE6fD1dsXBIUIUEFXpTnjPaAme1i2If1EMP/dBDH8qzH6sOX8KBAychJrHnPRuMB5qW3yZ4FTWjrWhgsXDhQvm1W7duRc4vXbpUlqEVRPlZe/t/BuNr165h9OjRMgipWrUqgoODsX//fgQEBFRw64mIbm/ur3/h11MJMphYPDQYNSuz7KnaZrQFzmqXjH1QDz30Qw99KOt+hF24hve2FCTbTeztj4cCaqEilPeMtuJLoe5GDCg3mzNnjjyIiNRq2/FYfP5bwV3ymY+3RqBfFaWbpDmc0SYiPSdrv3zTRqkvd20MvVBF8jYRkV6cjkvHhDVH5ePnuzTEk8GWLd+hApzRJiI9ys03yqAiIT0HzXwqYdZT+toolYEFEVEZSc3MkztrZ+Ya0LlxNUzp11zpJmkWZ7SJSI+m/+8EwmNS4OnqiCVDQ+Dhoq9fxZlJSERUBgxGE15dFYELVzNRp4ob5j0TBEcHfsQSEVGBVYdisOLPGJms/dm/2qFBdQ/oDUc9IqIyMOuX09j7VyJcneyxZFgwvD2clW4SERGpRHjMNUzbVLCz9r97+aN785rQIwYWRERW2hx5BYv2nJWPP34qEC1reyndJCIiUomE9IJk7VyDEX1a+uKVbvpJ1i6OgQURkRWi4tIwcW2kfPzig43waGBtpZtEREQqStYesyIc8Wk5aFqzEj55Wl/J2sUxsCAiKqWUzFy88F0YsvIMcmOjN/swWZuIiP4xY/NJHI6+hsoujnJPo0o6S9YujoEFEVEpk7VfW3UEMcmZ8PN2wxeD28HBXr93oYiIyDJrDl/E8oMXCpK1B7dFoxqVoHcMLIiISmH29n+StRcPCUEVdyZrExFRgSMXUzB143H5+PUezfBQcx/YAgYWREQW+vlYLBbsLkjW/ujJNgio7al0k4iISCUS03Pw0vKCZO1eAT4Y270JbAUDCyIiC/wdn45/ry3YWXvU/Q3xWNs6SjeJiIhUIs9QkKwdl5aNxjU8MPvpQNjb0DJZBhZERPcoLTsPLy4PQ8aNnbUn9WWyNhER/eP9LadwKDpZJmkvGRaCyq5OsCUMLIiI7oHRaMKE1UdwLilD7qwtkrW5szYREZmtC7uEZfuj5eM5g9qisQ0kaxfHUZGI6B7M23UGv55KgLOjPRYOCUK1Si5KN4mIiFQi8lIKpmw4Jh+P79EUPQNsI1m7OAYWRER3sSsqAXN+/Us+/u+AVmhTt4rSTSIiIpVIun4jWTvfiB4tauK1h5rCVjGwICK6gwtXMzBuVQRMJmBIx3p4OsRP6SYREZHKkrWvpGajUQ0PfDqorU0laxfHwIKI6Daycg0yWTstOx/t6lXBtIdbKt0kIiJSkQ+2nsKf528kaw8NgaeNJWsXx8CCiKgEJpMJk3+MRFRcOqpXcsbCZ4NlfgUREZHwY/glLP2jIFlblJVtUtP2krWL4yhJRFSCb/dHY+ORK3Cwt8O8Z4Lg6+WqdJOIiEgljl9OxeQfC5K1X3uoCXq39FW6SaqgaGAxc+ZM3HfffahcuTJq1qyJAQMG4PTp03d93dq1a9G8eXO4urqidevW2Lp1a4W0l4hsQ2h0Mv675ZR8PLlvc3RsVE3pJhERkUpcvZ4jl8nm5Bvxf81rYnyPZko3STUUDSz27NmDMWPG4ODBg9ixYwfy8vLQq1cvZGRk3PY1+/fvx+DBgzFy5EhERETIYEQcx48fr9C2E5E+JaRl4+UV4cg3mvBwm1oYeX9DpZtEREQqkW8wYuzKCFxOyULD6kzWLs4RCtq2bVuR58uWLZMzF2FhYXjwwQdLfM1nn32GPn36YOLEifL5jBkzZFAyb948LFq0qELaTUT6re4hBozE9Bw0rVkJHz3ZBnZ2HDCIiKjAzJ+jcODcVXg4O2Dx0GB4udl2sraqAoviUlNT5Vdvb+/bXnPgwAFMmDChyLnevXtj48aNJV6fk5MjD7O0tDT5VcyOiMMS5ustfZ3a6KEf7IN66KEf5rZ/9HMUDkUnw8PFAfP+FQhne5Om+mXNe6G2foqlsj/++COioqLg5uaGzp0746OPPoK/v/9dl8q+/fbbiI6ORtOmTeVr+vXrV2HtJiL92nTkCr7ed74wWbuZT2Wlm6Q6qgksjEYjxo8fjy5duqBVq1a3vS4uLg4+PkV3MxTPxfnbDU7Tp0+/5fz27dvh7u5eqraKGRI90EM/2Af10Ho/IpLs8O3fl+TjQfVzEXV4D6JgO+9FZmYm1MS8VFbk4eXn52PKlClyqezJkyfh4eFxx6Wy4nP/4YcfxsqVK+VS2fDw8DuOK0REd3MpA/h800n5eGz3JujTqpbSTVIl1QQWYgAReRL79u0r0587efLkIjMcYsbCz89PDlCenp4W39ETA3bPnj3h5KTdqS899IN9UA899CPqSgomLv5TPh59fwO82buZzb0X5tlcteBSWSJSi+SMXHx92kEma3fzr4HXe2pzjLCZwGLs2LHYvHkz9u7di7p1697xWl9fX8THxxc5J56L8yVxcXGRR3Fi0C3tL0HWvFZN9NAP9kE9tNqP6zn5GLf2BHKNdujYsCre6tsCjg72NvdeqP29K4+lskRE95Ks/fqaSCTn2KGetxs+G9ROliEnFQYWYgOqV199FRs2bMDu3bvRsOHdq6906tQJO3fulMumzMQdKXGeiMjSz6C31kfiXFIGvJxMmPN0G80HFXpUXktlBebhFcU+qIce+qGHPny47TT2n0uWOXdfPN0K7k7a7E9eBeXgOSq9/Emsgd20aZPcy8L84e/l5SWT9YRhw4ahTp06cs2sMG7cOHTt2hWzZ89G//79sWrVKoSGhmLJkiVKdoWINEjsmLolMhaO9nZ4zj8f1SvdOrtJ+l0qKzAPr2Tsg3rooR9a7UO4zL1zkI+fbWJE9NEDiD4KTdtRzjl4igYWCxculF+7detW5PzSpUsxYsQI+TgmJgb29v/cQRSVQUQwMnXqVJnMJ6p+iGluJuYRkSXCLiTjg60Fm+BN6tMMNa6dULpJVMFLZQXm4RXFPqiHHvqh5T6cik3HW1+K3DsjRnWph9bGc5rsR0Xn4Cm+FOpuxBKp4gYOHCgPIqLSSLqeg1du2gRvWMd6+PlnBhZqUlFLZZmHVzL2QT300A+t9eFaRi7GrDqC7DwjHmhaHf/u5Y9ftp3TXD+UyMFTRfI2EVFFMRhNGLcqAvFpOWhcw+PGJnh3v8lBFYtLZYlIqWTt11ZF4GJyFup5u+OLwUzWtgSzFInIpszZ8Rf+OHMV7s4OWDQkGB4uvL+iRmKprKgEJZbK1qpVq/BYvXp14TViqWxsbOwtS2VFIBEYGIh169ZxqSwRWWTW9tP4/e8kuDkV7Kxdxd1Z6SZpSqlG1PPnz+P333/HhQsXZEJHjRo10K5dOznd7OrqWvatJCIqA79FxWPerjPy8cwnWqMpd01VLS6VJaKKtjnyChbvOScfzxrYBi1qWZZnRRYGFitWrJAbEImpZVHCr3bt2nJKOjk5GWfPnpVBxbPPPou33noL9evXL79WExFZ6GJyJl5fXVDOY1in+nisbR2lm0RERCpxKjYNE9dGyscvPtgID7eprXST9B1YiBkJZ2dnWa1p/fr1smrGzUQdcLE5kVjTGhISggULFvCuERGpQk6+AWNWhiM1Kw+BflXwn/4tlG6SrnFWm4i0JCUzFy8uD0NWnkEma7/Zp7nSTdJ/YPHhhx/KHUxvR1TVEGthxfH+++8jOjq6rNpIRGSVGZtPIvJSKqq4O2H+M+3g4lhQl5zKFme1iUiLBT1eW3UEMcmZqFvVDZ//i8naFRJY3CmoKK5atWryICJS2qYjl/H9wRj5eM6gtqhbtXSbntGdcVabiLRo9vbT2PtXIlyd7GWydlUPJmtXeFWoZcuWlXg+Pz9fbjZERKQGZxLSMfnHY/Lx2O5N0N2/ptJN0i0xq/3nn3/ilVdeuSWouHlWe9GiRYiKikKjRo0UaScRkdnWY7FYsPusfCxKj7es7aV0k2wzsHjttdfknaZr164Vnjt9+jQ6dOiAH374oSzbR0RUKpm5+XITvMxcAzo1qobXezZTukm6ZumsdnBwcLm2h4joTk7HpePfawsKeox+oCELeigZWERERODSpUto3bq13NV0/vz5CAoKQvPmzXH0aMGbRESkZKnSqRuO46/466hR2QWfDW7LNbMViLPaRKRmqZl5eHF5qLzx1LlxNbzFZG1lA4vGjRvjjz/+wBNPPIE+ffrg9ddfx1dffSUT98SuqERESloTehE/RlyGiCXErqk1K7MSUUXirDYRqTlZe9zqCERfzUSdKm6Y90wQHB24X3RZKfXf5JYtW2QSnigfWKVKFXz99de4cuVKmTWMiKg0Tl5Jw7RNJ+TjN3r5o2MjFpKoaJzVJiK1mrPjL+w+nQgXx4JkbW8maysfWLz44ovybpQoGShqlUdGRspqIGIQWbNmTdm2kIjoHqVn58n9KnLyjejuXwMvd22sdJNsEme1iUiNth2PxbxdZ+TjD59sjVZ1+HmkisBCDBii+scbb7wBOzs7+Pr6YuvWrXjvvffw/PPPl3kjiYjuJa9i0o/HcD4pA7W9XPHp021hz7wKxXBWm4jU5O/4dLyxpmDG9PkuDfF4u7pKN0mXShVYhIWFITAw8JbzY8aMkd8jIqpo3x+8gC2RsXC0t8O8Z4NYi1xBnNUmIjVJzcrDC8vDkJFrQMdG3pjcj8naim+QV7we+e34+/tb0x4iIosdu5SKGZtPyceT+jZHUL2qSjfJpplntc03oMyz2iLXQsxqP/3000o3kYhshNFowuurjxTOZs9/JghOTNYuN/f8NyvWyR48ePCu16Wnp+Ojjz6SAwgRUUXciXplZRhyDUb0DPDByPsbKt0km8dZbSJSi7k7/8ZvUQk3krVDUK3S7W+OUwXOWIhp7SeffFIm3j3yyCMICQlB7dq14erqKksKnjx5Evv27ZN3pfr3749Zs2aVQfOIiO6cV/HWukhcTM5C3apu+OSpQJn3RcrirDYRqcEvJ+Lw+c6/5eMPHm+N1nWZrK2aGYuRI0fi3LlzmDJligwiXnjhBTzwwAO477775I6rX375JerVq4fDhw9j9erV8vHd7N27VwYpIkARvwxs3Ljxjtfv3r1bXlf8iIuLu9duEJGOLNsfjW0n4uDkYCent73cnZRuks3irDYRqcmZhH+StUd0boAng5msrbocC3EXasiQIfIQUlNTkZWVhWrVqsHJyfIBPSMjQ06XizW3oizhvRIbLXl6ehY+r1mzpsV/NhFp29GLKfhga0FexZR+LRDoV0XpJtk0zmoTkVqkZRcka1/PyUeHht74T/8WSjfJZpQqedtMDCDW1CTv27evPCwlAglRvpCIbDevQuxXkWcwoU9LX3k3ipQlZrXFTae1a9fKWeslS5bIm0+CmFkOCAiQs9tiVrtFCw7yRFR+ydoTVh/BucQM1BLJ2s8yWVu1gcXnn39e4nkRXDRr1kzWK68Ibdu2RU5ODlq1aoV3330XXbp0ue214jpxmKWlpcmveXl58rCE+XpLX6c2eugH+2C7/RB5Ff9ecxSXrhXkVbz/WAvk5+db9TP5XpRN38t6VpuIyFKf//Y3fj2VAGdHeywaEozqTNZWb2AxZ86cEs+npKTIAaRz58746aef4O3tjfJQq1YtLFq0SE6xi2BB7OTarVs3WdYwKCioxNfMnDkT06dPv+X89u3b4e7uXqp27NixA3qgh36wD7bXjz2xdtgR7QAHOxP+VTcd+3aV3Z9ry+9FZmZmmbfD2lltIiJL7DgZj7m/FiRrvz+gFZfIqj2wOH/+/G2/JxK7xV2qqVOnYsGCBSgPoprIzRVFRCBz9uxZGfAsX768xNdMnjwZEyZMKDJj4efnh169ehXJ07jXO3piwO7Zs6em777poR/sg232I/JSKv536JCYt8B/+rXA0I53LxJxL/he/DOba42yntUWBT5ELoYoURsbG4sNGzZgwIABdyzw0b1791vOi9eKvTSISL/OJl6XS6CE4Z3qY2CIn9JNsklW5VjcrFGjRvjwww9lInZFat++vUwIvNPUfEmlD8WgW9pfIKx5rZrooR/sg+30Q+RVjFsTKfMq+rbyxXP3Nyrz0rK2/F6URb/LelabBT6I6F6ki2Tt70KRnpOP9g28MfXhAKWbZLPKLLAQRInZii79euTIEblEioj0S+RVvLmuIK/Cz9sNHz3VhvtVqFBZz2qzwAcR3UuytigrezYxA76erpj3bDsma+slsDh27Bjq169/z9dfv34dZ86cKTIoiUBB3M0SQYpYxnT58mV899138vtz585Fw4YN0bJlS2RnZ8sci99++03mSxCRfn27Pxq/nIgv3K/C01X7swq2piJntS0p8EFE2jZ/1xlsPxkPZwd7LBoajJqVXZVukk1zLIs1uGKKW6yBfeONNzB8+PB7/nmhoaFF1sOacyHEz1i2bJlcFxsTE1P4/dzcXPlniGBDJF63adMGv/76a4lraolIHyIvpeD9m/araFOXd6K1qrxntUtT4IOVA4tiH9RDD/0o7z7sOp2IT3/9Sz5+95EWaOnrUS5/lq2/F3kWvMaiwEJMLd9u+YE4P2rUKEyaNOmef574wBdLHG5HBBc3e/PNN+VBRLazydHYlREyr6J3Sx/uV6Fxls5qV0SBD1YOLBn7oB566Ed59CEhC/j0mANMJjt08THCI/4otm4t2Gm7vNjqe5FpQdVAiwKLXbt2lXheJMk1bdpU7rCakJAgd1slIrKGuOkwaX0kYpIzUaeKGz5+MpB5FSpX1rPaFVHgg5UDi2If1EMP/SivPogdtQcu/hNZhgwE16uCJc+FyH0ryoutvxdpFlQNtCiw6Nq16x2/f/ToUTndbDAYLPmxRES3+P7gBWw9FgdHezvMe6YdvNy1+WFuS8p6VrsiCnywcmDJ2Af10EM/yrIP4qbT5FWROJOYAR9PFywcGgwPt4rZBM9W3wsnC64v0+RtIqKycPxyKmZsLsirmNS3OdrVq6p0k0iBWW0W+CCi4hbsPottJ+JkMY+FQ5isrTYMLIhIdfXIx64MR67BiB4tamLk/Q2VbhIpNKvNAh9EdLNdpxPwyfbT8vH0R1shiDedVIeBBRGphpzi/vEYoq9moraXKz4ZyLwKW8YCH0RkFp2UgXE/REB8JAxuXw/PdKindJPI2sAiMjLyrrudEhGV1g+HLmJzZKzMq/jimSBUcXdWuklERKSwjJx8vLg8DGnZ+WhXrwrefZQ7a+sisBCbDom7hyXdQTKf591FIiqNU7FpmP6/E/LxxN7+CK7PKW4iIlsnfrd8c10kTseno0ZlFywaEgwXRwelm0VlEViIxDkiovK4GzVmZThy8o3o7l8Dox9opHSTqBQ4q01EZW3RnnPYciy2IFn72SD4eDJZWzeBRXlubEREtns3aurG4ziXmAFfT1fMfrot7O0586lFnNUmorK0569EfPxLlHz8ziMtEdLAW+kmUVkGFh9//DFeffVVuLm5yed//PEHQkJCCmuAp6en46233sKCBQss+bFEZMPWhl7ChojLcJB5Fe3g7cG8Cq3irDYRlZULVzPw2o1k7UEhfniWydr6CyxEzfARI0YUBhZ9+/aVNcUbNWpUuOX34sWLGVgQ0T35Kz4d0346Lh9P6NkM9/FulKZxVpuIykJmbkGydmpWHtr6VcF7A1pytlMjLNr/vPj09p3KABIR3W3geGVFOLLzjHiwWQ283LWx0k2iMvT7779jyJAh6NSpk9xXQli+fDn27dundNOISAPJ2lFx6aheyQULhwQxWVuvgQURUVl5e+MJnEm4jpqVXfDp04HMq9CR9evXo3fv3nJ2OyIiAjk5OfJ8amoqPvjgA6WbR0Qq9uXv5wrLjougopZXwSoZ0gYGFkRU4daGXsT68EsQscTng9vJu1KkH//973+xaNEifPnll3Bycio836VLF4SHhyvaNiJSr31/J+HDn83J2gFcHmsLO29/9dVXqFSpknycn58vdz6tXr16YfI2EdHd8ire3vRPXkXHRtWUbhKVMVFW9sEHH7zlvJeXF1JSUhRpExGp28XkTIz9IRxGE/B0SF0M6cicLd0HFvXq1ZN3oMx8fX3lmtni1xAR3S2v4oGm1fFKtyZKN4nKgRgbzpw5gwYNGhQ5L/IrzMU+iIjMsnINeGF5GFIy8xBY1wvvPdaKydq2EFhER0eXX0uIyKbyKuYM4n4VejV69GiMGzcO33zzjfzl4MqVKzhw4ADeeOMNTJs2TenmEZHKkrXfWh+JU7FpqF7JGYuGBsPVicnaNhFYZGdn49dff8XDDz9cWH7WnJQnf5ijI9577z24unJXRCIqag3zKmzGpEmTYDQa8X//93+yDLlYFiX2O5o4cSJGjRqldPOISEW+3ncePx29IpO15z/DZG2bSt4W+RRinwqzefPmYf/+/bLqhzjEsihL9rDYu3cvHnnkEdSuXVve1dq4ceNdX7N7924EBQXJQapJkyayTUSkbqfj0jGNeRU2Q3ye/+c//0FycjKOHz+OgwcPIjExUeZYNGzYUOnmEZFK7D+ThA+2npKPp/ZvgQ4cG2wrsFixYgVeeOGFIudWrlyJXbt2yWPWrFlYu3btPf+8jIwMBAYGYv78+fe8q2v//v3RvXt3uTHf+PHj5d2vX375xZJuEFEFysgReRVhhftVMK9Cv8QMtpjJDgkJkRWgtm7dioCAAJw4cQL+/v747LPP8PrrryvdTCJSSbL2mJUFydpPBtXF8M5Fc7LIBpZCiWS81q1bFz4XS57s7f+JTdq3b48xY8bc888TO3eL416J8oXibtfs2bPl8xYtWshkwDlz5sia6USkvrWz/9lwDGcTM+Dj6YI53K9C10T+hJjV7tGjh5zNHjhwIJ577jk5YyE+t8VzBweunSaydSJZW+ysfS0zD23qeuH9x5msbZOBhSgTeHNOhZjavplYU3vz98uaSP4TA9bNREAhZi6ISH1WHb6IjUeuwMHeDvOeCUI15lXompix/u677/Doo4/KJVBt2rSRZcmPHj3KXxqIqPCG05QNx3AyNg3VPJyxaAiTtW02sKhbt64cLMSUdkkiIyPlNeUlLi4OPj4+Rc6J52lpacjKypK7vBYnAp2bgx1xrZCXlycPS5ivt/R1aqOHfrAP6u+HGDTe+emEfDyhRxO0rVNZtX3V+3thyWutcenSJQQHB8vHrVq1krlwYukTgwoiMvvmj2hsiLhceMOpdhUma9tsYNGvXz851S3yHIpXfhK/2E+fPl1+T01mzpwp21Xc9u3b4e7uXqqfuWPHDuiBHvrBPqizH9n5wKxjDsjNt0PLqkbUTjuFrTcS9NRMj+/FvRLVm6xlMBjg7OxcpFKgeUNVIqL9Z4sma3dqzGRtmw4spkyZgjVr1sgZi7Fjx6JZs2aFu6yKClFiyltcU56bLsXHxxc5J557enqWOFshiETCCRMmFJmx8PPzQ69eveTrLL2jJwbsnj17wsnJCVqlh36wD+rth5jmfnXVUSRlJ6C2lyuWvdQJVdzV3T+9vheWMM/mWkO89yNGjJAzFeYS5S+99BI8PDyKXPfjjz9a/WcRkbZcTsnC2JURMBhNeKJdHYxgsrYuWRRYiGVHIiHv5ZdflnXKxSAiiGluMZCJUrPFlyqVpU6dOskqIzcTg6g4fztigDMPcjcTg25pf4Gw5rVqood+sA/q64eoSf7LyQQ4OdhhwZBg1PAq3cygEvT2Xlj6GmsNHz68yPMhQ4ZY9fNESXJRbTAsLAyxsbHYsGEDBgwYcNeS5OJmkqhEJW4iTZ06VQY7RKSc7DyRrB2K5IxctKrjiQ+eaM0lkjplUWAhiKpM27Ztk/XJRZUoQewn4e3tbfEffv369cKfYS4nK8rIip9Vr149Odtw+fJlmQwoiDtfYmbkzTffxPPPP4/ffvtNzqBs2bLF4j+biMpe2IVkzCyc5g5AW78qSjeJKtDSpUvL9OeZS5KLz/snnnjinkuSi7FClEffuXOnLEleq1YtVg4kUoi4Bz3tp5M4fjkN3kzW1j2LAwsz8cu/KC9rjdDQULknhZl5yZK46yU2vhN3qGJiYooENSKIEMmAoh66SBT/6quvOGAQqcDVjFyMWRGBfKMJD7ephWGd6ivdJNI4liQn0r7f4+ywITr2RrJ2O9Stqp1ZbKrAwKIsdOvWrXA5VUlK2lVbvEbs8k1E6iE2OJqwNhJxadloVMMDHz7ZhtPcVOFKU5KclQOLYh/UQw/92P93AjZEF+x39lbvZrivnpcm+6OH9yKvgqoGKhpYEJE+/HzRHvsvJ8PNyUFOc1dy4UcLVbzSlCRn5cCSsQ/qodV+XMsBPol0gBF2CK5uRM1rJ7B1a0EJcq3S6ntRkVUDOfoTkVV2nU7E9ssFd6RmPtEazXwqK90konvGyoFFsQ/qoeV+5OQZMPjrw7ien4Y67iYsGd0Nnu5FtynQEi2/FxVdNZCBBRGV2sXkTExcf0w+HtLBDwPa1VG6SWTDSlOSnJUDS8Y+qIfW+iF31t54Escup6GKmxNG+mfJoEJLfdDLe6FE1cCC24xERKUqHxiG1Kx81K9kwqQ+/ko3iWycKD0uKkFZUpKciMrW9wcvYG3YJdjbAXMHtUE17U5UUCkwsCCiUt2R+s+G4zgZK8oHOuG5Zga4OPLjhMqWKEkuSpCL4+aS5OZqgWIZ07BhwwqvF2Vmz507J0uSR0VFyb2VRElyUUmQiMrfofPJmP6/k/LxpL7N0YU7a9sc/iZARBZb8WcM1offuCP1dBtUvXUlCZHVREnydu3ayUMQuRDi8bRp0+Tz25UkF7MUYv8LUXaWJcmJKkZsahZeWRFWWHJ89AONlG4SKYA5FkRkkfCYa5j+v4LKHm/2aY5Ojapha5TSrSI9YklyIu0sjX3p+3AkXc9Fc9/K+Pgplhy3VZyxIKJ7lpCWjZeWhyHPYELfVr548UHekSIismUi+H9n0wkcvZgCLzcnLBkaAndn3re2VQwsiOie5OYb8cqKcCSk56BpzUqYNTCQd6SIiGycWBq7OvSiXBr7xeB2qFeNO2vbMgYWRHRP/rvlJEIvXENlV0csGRbCTfCIiGxcaHRykaWxDzaroXSTSGEMLIjorlYfjsF3By7Ix3MHtUXD6h5KN4mIiBQUl5ot8yrE0tj+rWtxaSxJDCyI6I7CLiRj6sbj8vHrPZrh/1r4KN0kIiJSUE6+AS+vCEPS9Rz4+zBZm/7BwIKI7umOVJ+Wvnj1oSZKN4mIiBT27k8nEBGTAk9XRyweGgwPLo2lGxhYENEddtYORWJ6wR2p2U8Hwl5k5xERkc1a+WcMfjh0EWKC4vPB7dCAS2PpJgwsiKjE8oFvrY/E0UupqOLuhC+HhfCOFBGRjQu7cA3v/FSwNPbfvfzRzb+m0k0ilWFgQUS3WLD7LDYduQJHezsseCaI5QOJiGxcfFo2Xv7+n32MXunWWOkmkQoxsCCiIn45EYdZv5yWj999tCU6N6mudJOIiEjhfYxEUMF9jOhuGFgQUaETV1Lx+uoj8vGwTvUxpGN9pZtEREQKE3tVhMekcB8juisGFkRUOM09clkoMnMN6NKkGt5+OEDpJhERkcJWHYqRu2vLZO1/teM+RqT+wGL+/Plo0KABXF1d0aFDBxw6dOi21y5btkxOv918iNcRUell5uZj5LeHEZeWjcY1PLDg2WA4Oaji44GIiBQSHnMN0zYV7Kw9oUczdG/OZG26M8V/c1i9ejUmTJiAd955B+Hh4QgMDETv3r2RkJBw29d4enoiNja28LhwoWBHYCKynMFowvhVR3D8chqqeThj6Yj28HJzUrpZRESkoIT0gmTtXIMRvVv6YEx37mNEGggsPv30U4wePRrPPfccAgICsGjRIri7u+Obb7657WvELIWvr2/h4ePDnYCJSuu/W05i+8l4ODvaY8mwYFaAIiKycSJZe8yKcMSn5aBJzUqY/XRb7mNE90TR7Jvc3FyEhYVh8uTJhefs7e3Ro0cPHDhw4Lavu379OurXrw+j0YigoCB88MEHaNmyZYnX5uTkyMMsLS1Nfs3Ly5OHJczXW/o6tdFDP9iHsrF0/wUs/SNaPv74iVZoU7uyTf6/0EMfrO2H1vtORGVnxuaTOBx9DZVdHLFkaDCTtemeKfovJSkpCQaD4ZYZB/E8KiqqxNf4+/vL2Yw2bdogNTUVn3zyCTp37owTJ06gbt26t1w/c+ZMTJ8+/Zbz27dvlzMjpbFjxw7ogR76wT6U3pGrdlj2l5i0tMOj9QywuxiOrRdL//P4Xmi7H5mZmeXSFiLSljWHL2L5wYIl5nMGtUWjGpWUbhJpiOZC0E6dOsnDTAQVLVq0wOLFizFjxoxbrhezISKH4+YZCz8/P/Tq1Uvmalh6R08M2D179oSTk3bXoOuhH+yDdcSdqJXfhsEEI55t74d3Hm5e6prkfC/00Q/zbC4R2a4jF1MwdWPBztqv92iGHgFcak4aCiyqV68OBwcHxMfHFzkvnovciXshBs927drhzJkzJX7fxcVFHiW9rrS/QFjzWjXRQz/YB8tFxaXhpRURyMk3okeLmpj+WCs4lkEFKL4X2u6HHvpNRKWXmJ6Dl5YXJGv3DPDBqw8xWZs0lrzt7OyM4OBg7Ny5s/CcyJsQz2+elbgTsZTq2LFjqFWrVjm2lEgfLl3LxPBvDiEtOx8h9avii8FBZRJUEBGRduUZjBizMlyWHG9UwwOfPh3IZG0qFcV/oxDLlL788kt8++23OHXqFF5++WVkZGTIKlHCsGHDiiR3v/feezI/4ty5c7I87ZAhQ2S52VGjRinYCyL1u3o9B8O+OSSrfDTzqYSvhofAzdlB6WYR3RH3OSIqf+9vOYVD55NlkvaSoSGo7MoZTNJojsWgQYOQmJiIadOmIS4uDm3btsW2bdsKE7pjYmJkpSiza9euyfK04tqqVavKGY/9+/fLUrVEVLLUrDwZVJxLzEBtL1d8+3x7VHF3VrpZRPe0z5EoQy6Cirlz58p9jk6fPo2aNUveqEvkzonvm5U2d4jIVqwLu4Rl+6MLk7VFeVkizQYWwtixY+VRkt27dxd5PmfOHHkQkQW7ai87jBNXCjbAWz6qA2p5uSndLCKL9jkSRICxZcsWWRlw0qRJd9zniIju7tilVEzZcEw+Hvd/TWVuBZHmAwsiKh85+Qa89H04Qi9cQ2VXR3w3sj0as3QgaUBF7HMkcK+jotgH2+nH1YxcvLA8VG6G95B/DbzyYIMy/7P4XtjePkcMLIh0SgwWr3wfjr1/JcLNyQHLnrsPLWt7Kd0sItXscyRwr6OSsQ/67ofBCCw4ZY/YNHvUdDWhl2cstm2LRXnhe2E7+xwxsCDSaYWPsSvDsTMqAS6O9vh6eAiC63sr3SwiVe1zJHCvo6LYB9vox/tbo3AmLQYezg74dnSHcsur4Hthe/scMbAg0mFQMW5VBLafjIezoz2+HBaCzk2qK90sItXtcyRwr6OSsQ/67ceGiEtYdiBGPp79dFu0qFMV5Y3vhe3sc6R4uVkiKtvlT2KmYuuxODg72GPx0GA82KyG0s0ishj3OSIqe8cvp2LS+oJk7bHdm6BPKxY6oLLFGQsincjOM+CVFeH4LSpBzlQsGhKE7v4ll+Qk0gKxRGn48OEICQlB+/btZbnZ4vsc1alTR+ZJmPc56tixI5o0aYKUlBTMmjWL+xwR3ZCckYsXl4chJ9+I7v418HrPZko3iXSIgQWRTkrKigHj97+T4OpkLzc44kwFaR33OSIqG/k38u4up2ShQTV3zP1XOzhwZ20qBwwsiDQuJTMXzy87jPCYFLg7O+Dr4fehU+NqSjeLqExwnyMi6334cxT2n70qx4glw0Lg5abtPAFSLwYWRBoWn5aNYV8fwun4dHi6OmLpc/ex+hMRERXadOQyvtp3Xj7+ZGAgmvlUVrpJpGMMLIg06mzidYxYeggXk7NQs7KL3Pyuua9l5TGJiEi/TlxJxVvrI+XjV7o1Rr/WLGRA5YuBBZEGHY5OxujvQpGSmYf61dzx/cgO8PMu3WZeRESkP9duJGtn5xnRtVkNvNHLX+kmkQ1gYEGkMZsjr2DCmqOytGygXxW5+V31SrfW4SciIttN1n71hwhcupaFet7u+JzJ2lRBGFgQaYTRaMLnv/2Nub/+LZ/3CvDBZ/9qBzdnB6WbRkREKjLrl9PYdyYJbk4iWTsYXu5M1qaKwcCCSAMycvLx77VH8fPxOPn8+S4N8Z/+LXgHioiIivjp6BUs3ntOPp41sA1z76hCMbAgUrkLVzPkOtmouHQ4Odjh/QGt8fR9fko3i4iIVOZUbBreXHdUPn6pa2M83Ka20k0iG8PAgkjFth2Pw8S1R5Geky/zKBYPDWI5WSIiKnFPoxeWh8pk7QeaVsfE3kzWporHwIJIhURi9sfbogprj4fUr4p5zwTB18tV6aYREZHKGIwmmawtyo/7ebsxWZsUw8CCSGXOJKRj3KojOHElTT5/4cFG8s6Tk4O90k0jIiKVJmv//veNZO2hIajq4ax0k8hGqeI3lfnz56NBgwZwdXVFhw4dcOjQoTtev3btWjRv3lxe37p1a2zdurXC2kpUnlWflh+IRv/P98mgoqq7E5YMDcaUfi0YVBARUYm2RMZi0Z6z8vFHT7VBi1pM1iblKP7byurVqzFhwgS88847CA8PR2BgIHr37o2EhIQSr9+/fz8GDx6MkSNHIiIiAgMGDJDH8ePHK7ztRGUlOikDz3x1EG9vOoGc/IL1sb+MfxC9Wvoq3TQiIlKpqLg0WTHQPLv9aCCTtcnGA4tPP/0Uo0ePxnPPPYeAgAAsWrQI7u7u+Oabb0q8/rPPPkOfPn0wceJEtGjRAjNmzEBQUBDmzZtX4W0nspbBCHy1Lxp9PtuLg+eS5TT2O48E4Nvn2qOmJ/MpiIioZKmZebJiYFaeAfc3qY43maxNtp5jkZubi7CwMEyePLnwnL29PXr06IEDBw6U+BpxXsxw3EzMcGzcuLHE63NycuRhlpZWsG49Ly9PHpb439HLOJpkB+PRy3BxcoKDgx2c7O1kgpSjeOxgDyd7e1kSVD52LPjqLA5He7g42sPR3g52dsomVJn7bWn/1UQPfdj3VwI+jnRAXNZf8nnnRt6Y8ViA3CXVYMiHwQBN0MN7oYc+WNsPrfedyNaStV9bFYELVzNRt6obvhjcDo5cMku2HlgkJSXBYDDAx8enyHnxPCoqqsTXxMXFlXi9OF+SmTNnYvr06bec3759u5wZscR/Djkgy+CAb/8+gdKygwlO9oCjPeRXcTibDweT/OriALjI54CreOxgkl/dxHNH8dUEN/kVcHcs+FmlsWPHDmidFvuQlA38L8YeR66KN84OHo4mPFrfiA41EnD8YAK0uqhPi++FHvtQ2n5kZmaWS1uIqOx9uuM09vyVCFcneyweGsxkbVIN3VeFErMhN89wiBkLPz8/9OrVC56eliU4bUoOx8XYBHhVqQoj7JBvMCLfaEK+wYR8oxF5BhPyDAVfzc9F2VBxjZkJdsg1Qh63Kt1MhoezA7zcnFDF3QlV3Z1l0q+3hzOqiaNSwdfqlZxRo7KL3AvB3mSQv3j07NkTTk5O0CJxd1VrfUi6noP5u89h1dFL8t+EqATYxceIj4c+iOqelgW5aqLF90KPfbC2H+bZXCJSt5+PxWL+rhvJ2k+2QcvaXko3iUgdgUX16tXh4OCA+Pj4IufFc1/fkpNWxXlLrndxcZFHcWLQtXTgXTwkSFag6tevg0WvFVOWIuDIyTMix2Ao+JpvkJvYiLWRWbkG+TU7z4CMHAMyc/ORmSse5+N6Tn7h17TsfKTLIw9pWXly0zSTCcgQ1+YacCU1+57aIwIPVzhgfVIkanm5y70RaldxRS0vN9Su4oY6VdzgJqZLNKA072NFi0vNxpe/n8PKP2Pk+yx0bVYDb/RogvMRv8ugQu190Mt7YQt9KG0/9NBvIr37Kz4db9xI1h51f0M81raO0k0iUk9g4ezsjODgYOzcuVNWdhKMRqN8Pnbs2BJf06lTJ/n98ePHF54Td+jEebUSORgO9g5wdRK/rJfd4C0CluvZ+biWmSuPlKw8XMvIRfKN4+r1XHmXPCkjF0npOUhMz0GuwYhrmWIttR1i/74KQBy3EjMcdaq6y7WbflXd5YY7Yu1/g2oeqOXlyrWc91itY9kf0fgx/LL8excC/apgUp/m6NS4mry7fD5C6VYSEZEWpGbl4YXvQuWNx86Nq2FS3+ZKN4lIfUuhxDKl4cOHIyQkBO3bt8fcuXORkZEhq0QJw4YNQ506dWSuhDBu3Dh07doVs2fPRv/+/bFq1SqEhoZiyZIlsDUiYPFyd5JHA3jc9XqTySQ/mC4nX8f/du5D/eZtkHg9D7GpWYhNzcaVlCxcvpYlZz+SZFCSi6MXU275OSIB3c/bHfWrFQQaDaq5o2GNSmhU3UPOeNjybp9i1unXU/FYfuAC/jyfXHi+fUNvjO3eRJaRVTp5n4iItEXcSBy/KgLRVzPlqoJ5zwTxBh+pkuKBxaBBg5CYmIhp06bJBOy2bdti27ZthQnaMTExslKUWefOnbFy5UpMnToVU6ZMQdOmTWVFqFatWinYC20Qv9BWcXeGh1NltKhiQr+gOrcsfxDBR1pWPi5ey8Sla1m4dC0TF5MzEXPjuHgtS+aNnE/KkAeQWOT1ogKWCDga1fBAoxvBhvjauIaH/LP1urFdeMw1bIi4jP8dvSKXrAkiwOrd0gfPd2mIkAbeSjeTiIg0au6vf2HX6URZXVIka4s8SiI1UjywEMSyp9stfdq9e/ct5wYOHCgPKp/go2AWxAut6niV+Et0XFo2oq9myDJ3YmM3c5AhnoslP38nXJcHUDQXRnwQFgQaBcFGw+oeMuAQsx8ujtrI6TATeS+Hzidj+8l47DgZL5ecmYmlYk8G1cWzHevJvBUiKr358+dj1qxZ8saT2ED1iy++kLPbt7N27Vq8/fbbiI6OljeePvroI/Tr169C20xUlsQ488VvZ+TjD59sXeLYTKQWqggsSDvs7e3kcidxdG5861StWE51LikD5xKv41xiBs4lFXwVS63MuR+hF64V/Zl2QN2q7mhQvWBZVf1qHqh/Y6mVCDoKclOUJXJXjlxMQUTMNbmRnZihuLnaV2UXR/QM8MGTwXXRsVE1m14ORlRWVq9eLZfLio1TO3ToIJfKin2LTp8+jZo1a95y/f79+zF48GC5dPbhhx+Ws9sify88PJyz2qRJlzOA+esLipCL2e/H29VVuklEd8TAgsqM+GVaBALiEFWPbiYqXYkA4+yNgEPMcIig43xihszpMC+12lvCzxVlcsWa0jpV3VDbyxW+Xm6o4eGIs2mQMye1qlaSJXetzV0QlbtEBSfzErCziRn4Oz4dfyWk42Jy1i3Xi8R20c/eLX1lMCE2QSSisvPpp59i9OjRhTl3IsDYsmULvvnmG0yaNOmW6z/77DP06dMHEydOlM9nzJghi3vMmzdPvpZIK0TlyPm/ncX8Yw4wmAzo2MgbU/oxWZvUj4EFVQh3Z0c5fVt8ClfkdIhqVWKW48LVDJmYJpZXyUDjaqYsqSu+Lw4xY1CUIz4/8UdhbodcwuXmhMqujnB3dpB/pnm3c3OSm1jKZTCZZJK1uaSvqKYlKmiJxPY7EUu42vpVQUh9b9zfpDrqVdPu3hNEapebm4uwsDC5F5GZyLfr0aMHDhw4UOJrxPmb9y0SxAyHyMO7nZycHHkU389DVG2zZDfyfWeuYnPkFVy+bI+9Px4rkhuoJaIyI/ugvLAL13AuSWxaaYf7G3tj9sA2MBkNyDMWlCzXCvP/IUv+L6mRHvqRZ0UfLHkNAwtSlJhlqOnpKg9x17940JGSmYfLKWIGIUt+jU3Jkjke4mt0/DVkmRxlgCByO8wBiDVEgCJmRsRshFiK5e9TGU19KqOFr6cMXIioYiQlJcFgMBQW8jATz6Oiokp8jcjDKOl6cf52xLKp6dOn33J++/btcHe/95sHu2PtsCFaLNu0BxJioW3sgxpUdjLhiQZGtKuWgIN7foWWiZlDPdBDP3aUog+ZmSLIvTcMLEjVQUdVD2d5FJ/pENFzwWaFvZFnspN7c6Rk5iI1s2DjQLHpoAg4xHRywQaFBfkQYuLC3s5O5m14uDjAzckRnm6OqFHJBdUquaCKm5PMIyEi2yBmRG6e5RAzFn5+fujVqxc8PT3v+efUvZSK+n8n4syZv9GkSVM4aPROucFoZB9UwMPFEX0DquPQvt3o2bOnZjewFGO1+EVWy33QSz/yrOiDeSb3XjCwIM0TS57EIfIwiEgfqlevDgcHB8THF60uJ577+vqW+Bpx3pLrBRcXF3lYu3t5cMPqaFPXC1uz/kK/7k00/csH+6AO5uUnlv5bVCM99EEv/XAqRR8suV6boTwREemas7MzgoODsXPnziJr58XzTp06lfgacf7m6wVxh+521xMRUdnijAUREamSWKI0fPhwhISEyL0rRLnZjIyMwipRw4YNQ506dWSehDBu3Dh07doVs2fPRv/+/bFq1SqEhoZiyZIlCveEiMg2MLAgIiJVGjRoEBITEzFt2jSZgN22bVts27atMEE7JiamSNWfzp07y70rpk6diilTpsgN8kRFKO5hQURUMRhYEBGRao0dO1YeJdm9e/ct5wYOHCgPIiKqeMyxICIiIiIiqzGwICIiIiIiq9ncUiix6ZqlNXlvLv0mNgkRr9VyuTE99IN9UA899EMPfbC2H+bPRPNnpK2y9TGCfVAPPfRDD33QSz/yKmh8sLnAIj09XX4VGyAREdGtn5FeXkU3pLQlHCOIiEo/PtiZbOz2lKiDfuXKFVSuXFnu7GwJ846sFy9etGhHVrXRQz/YB/XQQz/00Adr+yGGAjFo1K5du0ilJVtj62ME+6AeeuiHHvqgl36kVdD4YHMzFuIvpG7dulb9DPGGaPUflt76wT6ohx76oYc+WNMPW56pMOMYUYB9UA899EMPfdBLPzzLeXyw3dtSRERERERUZhhYEBERERGR1RhYWMDFxQXvvPOO/KpleugH+6AeeuiHHvqgp35olR7+/tkH9dBDP/TQB730w6WC+mBzydtERERERFT2OGNBRERERERWY2BBRERERERWY2BBRERERERWY2BRSo8++ijq1asHV1dX1KpVC0OHDpWbKmlJdHQ0Ro4ciYYNG8LNzQ2NGzeWiT25ubnQkvfffx+dO3eGu7s7qlSpAq2YP38+GjRoIP8NdejQAYcOHYKW7N27F4888ojcMEdsJLZx40ZozcyZM3HffffJzdBq1qyJAQMG4PTp09CShQsXok2bNoW1yTt16oSff/5Z6WbZPK2PEXoZH7Q6RnB8UJ4exgclxggGFqXUvXt3rFmzRv4jW79+Pc6ePYunnnoKWhIVFSV3mV28eDFOnDiBOXPmYNGiRZgyZQq0RAx0AwcOxMsvvwytWL16NSZMmCAH6vDwcAQGBqJ3795ISEiAVmRkZMh2iwFQq/bs2YMxY8bg4MGD2LFjB/Ly8tCrVy/ZN60Qm7l9+OGHCAsLQ2hoKB566CE89thj8v80KUfrY4RexgctjhEcH9RBD+ODImOEqApF1tu0aZPJzs7OlJuba9Kyjz/+2NSwYUOTFi1dutTk5eVl0oL27dubxowZU/jcYDCYateubZo5c6ZJi8RHyYYNG0xal5CQIPuyZ88ek5ZVrVrV9NVXXyndDNLZGKHl8UFLYwTHB3XSy/hQ3mMEZyzKQHJyMlasWCGnWp2cnKBlqamp8Pb2VroZuibunok7Bz169Cg8Z29vL58fOHBA0bbZOvHvX9Dq/wGDwYBVq1bJO2piupvUQS9jBMeH8sfxQb20Pj5U1BjBwMIKb731Fjw8PFCtWjXExMRg06ZN0LIzZ87giy++wIsvvqh0U3QtKSlJ/uf28fEpcl48j4uLU6xdtk4s+xg/fjy6dOmCVq1aQUuOHTuGSpUqyY2PXnrpJWzYsAEBAQFKN8vm6WmM4PhQMTg+qJOWx4eKHiMYWNxk0qRJMsnoTodYd2o2ceJEREREYPv27XBwcMCwYcPE0jJorR/C5cuX0adPH7kOdfTo0dBiH4isIdbSHj9+XN7N0Rp/f38cOXIEf/75p1xHPnz4cJw8eVLpZumOHsYIPYwPAscIqkhaHh8qeozgzts3SUxMxNWrV+94TaNGjeDs7HzL+UuXLsHPzw/79+9XfAmCpf0QlUq6deuGjh07YtmyZXLaVYvvhWi7uKOQkpICtU91i+ok69atk1UmzMR/dNF2Ld7VFIO4uANyc3+0ZOzYsfLvXVQyEVVwtE4smxBVfETiLZUdPYwRehgf9DxGcHxQH72ND+U9RjiW+U/UsBo1asijtNNkQk5ODrTUD3EnSlQvCQ4OxtKlS1UzaFjzXqidGOjE3/fOnTsLP2jFvx/xXHyAUcUR91VeffVVOejt3r1bN4OG+Pekhs8ivdHDGKGH8UHPYwTHB/XQ6/hQ3mMEA4tSEFNJhw8fxv3334+qVavKMoJvv/22jP6Unq2whBg0xJ2o+vXr45NPPpF3gMx8fX2hFWLtskiOFF/F2lQx3Sc0adJErilUI1FKUNyBCgkJQfv27TF37lyZTPXcc89BK65fvy7XXZudP39e/t2LxDZRv18r09srV66Ud6NErXLzGmYvLy9Zu18LJk+ejL59+8q/8/T0dNkfMQj+8ssvSjfNZulhjNDL+KDFMYLjgzroYXxQZIwol1pTOhcZGWnq3r27ydvb2+Ti4mJq0KCB6aWXXjJdunTJpLXSe+KfQEmHlgwfPrzEPuzatcukZl988YWpXr16JmdnZ1le8ODBgyYtEX+/Jf29i/dDK27371/839CK559/3lS/fn3576hGjRqm//u//zNt375d6WbZND2MEXoZH7Q6RnB8UJ4exgclxgjmWBARERERkdXUs2CSiIiIiIg0i4EFERERERFZjYEFERERERFZjYEFERERERFZjYEFERERERFZjYEFERERERFZjYEFERERERFZjYEFERERERFZjYEFERERERFZjYEFERERERFZjYEFERERERFZjYEFUQVLTEyEr68vPvjgg8Jz+/fvh7OzM3bu3Klo24iISDkcH0jr7Ewmk0npRhDZmq1bt2LAgAFywPD390fbtm3x2GOP4dNPP1W6aUREpCCOD6RlDCyIFDJmzBj8+uuvCAkJwbFjx3D48GG4uLgo3SwiIlIYxwfSKgYWRArJyspCq1atcPHiRYSFhaF169ZKN4mIiFSA4wNpFXMsiBRy9uxZXLlyBUajEdHR0Uo3h4iIVILjA2kVZyyIFJCbm4v27dvLtbNiDe3cuXPldHfNmjWVbhoRESmI4wNpGQMLIgVMnDgR69atw9GjR1GpUiV07doVXl5e2Lx5s9JNIyIiBXF8IC3jUiiiCrZ79255B2r58uXw9PSEvb29fPz7779j4cKFSjePiIgUwvGBtI4zFkREREREZDXOWBARERERkdUYWBARERERkdUYWBARERERkdUYWBARERERkdUYWBARERERkdUYWBARERERkdUYWBARERERkdUYWBARERERkdUYWBARERERkdUYWBARERERkdUYWBARERERkdUYWBAREREREaz1/3ElFUyC+eqvAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GELU和ReLU激活函数\n",
    "gelu, relu = nn.GELU(), nn.ReLU()\n",
    "\n",
    "# 一些示例数据\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} 激活函数\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd01662-14cb-43fd-bffd-2d702813de2d",
   "metadata": {},
   "source": [
    "- 如我们所见，ReLU是一个分段线性函数，如果输入为正则直接输出输入；否则输出零\n",
    "- GELU是一个平滑的非线性函数，近似于ReLU，但对负值有非零梯度（除了大约-0.75处）\n",
    "\n",
    "- 接下来，让我们实现小型神经网络模块 `FeedForward`，我们稍后将在LLM的transformer块中使用它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9275c879-b148-4579-a107-86827ca14d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcaacfa-3cfc-4c9e-b668-b71a2753145a",
   "metadata": {},
   "source": [
    "- `FeedForward` 模块是一个小型神经网络，包含两个 `Linear` 层和一个GELU激活函数\n",
    "- 第一个线性层将输入从 `emb_dim` 扩展到 `4 * emb_dim` 维度\n",
    "- 第二个线性层将其投影回原始的 `emb_dim` 维度\n",
    "- 这种扩展然后收缩的模式在transformer架构中很常见"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcaacfa-3cfc-4c9e-b668-b71a2753145a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/09.webp?12\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c4976e2-0261-418e-b042-c5be98c2ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7c4976e2-0261-418e-b042-c5be98c2ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 3, 768])\n",
      "输出形状: torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "# 输入形状：[batch_size, num_token, emb_size]\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(\"输入形状:\", x.shape)\n",
    "print(\"输出形状:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcaacfa-3cfc-4c9e-b668-b71a2753145a",
   "metadata": {},
   "source": [
    "- 如我们所见，前馈网络保持输入的形状，这对于在transformer块中使用很重要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8756c5-6b04-443b-93d0-e555a316c377",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/10.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da2a50-04f4-4388-af23-ad32e405a972",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/11.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcb905-53c7-4886-87d2-4464c5fecf89",
   "metadata": {},
   "source": [
    "## 4.4 添加快捷连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae416c-821e-4bfa-a741-8af4ba5db00e",
   "metadata": {},
   "source": [
    "- 接下来，让我们讨论快捷连接背后的概念，也称为跳跃连接或残差连接\n",
    "- 最初，快捷连接是在计算机视觉的深度网络（残差网络）中提出的，用于缓解梯度消失问题\n",
    "- 快捷连接为梯度在网络中流动创建了一条替代的更短路径\n",
    "- 这是通过将一层的输出添加到后面一层的输出来实现的，通常跳过中间的一层或多层\n",
    "- 让我们用一个小的示例网络来说明这个想法：\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/12.webp?123\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cfd241-a32e-4601-8790-784b82f2f23e",
   "metadata": {},
   "source": [
    "- 在代码中，它看起来像这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "05473938-799c-49fd-86d4-8ed65f94fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), nn.GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), nn.GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # 计算当前层的输出\n",
    "            layer_output = layer(x)\n",
    "            # 检查是否可以应用快捷连接\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    # 前向传播\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    \n",
    "    # 计算简单损失\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} 的梯度均值: {param.grad.abs().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39bf277-b3db-4bb1-84ce-7a20caff1011",
   "metadata": {},
   "source": [
    "- 让我们首先打印**没有**快捷连接的梯度值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c75f43cc-6923-4018-b980-26023086572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight 的梯度均值: 0.000202\n",
      "layers.1.0.weight 的梯度均值: 0.000120\n",
      "layers.2.0.weight 的梯度均值: 0.000715\n",
      "layers.3.0.weight 的梯度均值: 0.001399\n",
      "layers.4.0.weight 的梯度均值: 0.005050\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837fd5d4-7345-4663-97f5-38f19dfde621",
   "metadata": {},
   "source": [
    "- 接下来，让我们打印**有**快捷连接的梯度值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "11b7c0c2-f9dd-4dd5-b096-a05c48c5f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight 的梯度均值: 0.221868\n",
      "layers.1.0.weight 的梯度均值: 0.207093\n",
      "layers.2.0.weight 的梯度均值: 0.329239\n",
      "layers.3.0.weight 的梯度均值: 0.266777\n",
      "layers.4.0.weight 的梯度均值: 1.326806\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff783a-46f0-49c5-a7a9-26a525764b6e",
   "metadata": {},
   "source": [
    "- 如我们从上面的输出中看到的，快捷连接防止了早期层（朝向 `layer.0`）中的梯度消失\n",
    "- 接下来当我们实现transformer块时，我们将使用这种快捷连接的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae578ca-e564-42cf-8635-a2267047cdff",
   "metadata": {},
   "source": [
    "## 4.5 在transformer块中连接注意力和线性层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3daac6f-6545-4258-8f2d-f45a7394f429",
   "metadata": {},
   "source": [
    "- 在本节中，我们现在将之前的概念组合成所谓的transformer块\n",
    "- transformer块将前一章的因果多头注意力模块与线性层、我们在前面部分实现的前馈神经网络结合起来\n",
    "- 此外，transformer块还使用dropout和快捷连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0e1e8176-e5e3-4152-b1aa-0bbd7891dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果本地没有 `previous_chapters.py` 文件，\n",
    "# 您可以从 `llms-from-scratch` PyPI 包中导入它。\n",
    "# 详情请见：https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# 例如，\n",
    "# from llms_from_scratch.ch03 import MultiHeadAttention\n",
    "\n",
    "from previous_chapters import MultiHeadAttention\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意力块的快捷连接\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # 形状 [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # 将原始输入加回来\n",
    "\n",
    "        # 前馈块的快捷连接\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # 将原始输入加回来\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b64d16-94a6-4d13-8c85-9494c50478a9",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/13.webp?1\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2d375-87bd-4153-9040-63a1e6a2b7cb",
   "metadata": {},
   "source": [
    "- 假设我们有2个输入样本，每个有6个token，其中每个token是一个768维的嵌入向量；那么这个transformer块应用自注意力，然后是线性层，产生类似大小的输出\n",
    "- 您可以将输出视为我们在前一章中讨论的上下文向量的增强版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3fb45a63-b1f3-4b08-b525-dafbc8228405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 4, 768])\n",
      "输出形状: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)  # 形状：[batch_size, num_tokens, emb_dim]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"输入形状:\", x.shape)\n",
    "print(\"输出形状:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e4ee4-cf23-4583-b1fd-317abb4fcd13",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/14.webp?1\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46618527-15ac-4c32-ad85-6cfea83e006e",
   "metadata": {},
   "source": [
    "## 4.6 编码GPT模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7d03d-9ff3-4ca3-ad67-01b67c2f5457",
   "metadata": {},
   "source": [
    "- 我们快到了：现在让我们将transformer块插入到本章开头编码的架构中，以便获得一个可用的GPT架构\n",
    "- 注意transformer块被重复多次；在最小的124M GPT-2模型的情况下，我们重复它12次："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b362d-f8c5-48d2-8ebd-722480ac5073",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/15.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e4b5d-ed89-4fdf-9a52-67deee0593bc",
   "metadata": {},
   "source": [
    "- 相应的代码实现，其中 `cfg[\"n_layers\"] = 12`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c61de39c-d03c-4a32-8b57-f49ac3834857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # 形状 [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750270f-c45d-4410-8767-a6adbd05d5c3",
   "metadata": {},
   "source": [
    "- 使用124M参数模型的配置，我们现在可以如下实例化这个具有随机初始权重的GPT模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ef94fd9c-4e9d-470d-8f8e-dd23d1bb1f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入文本: tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "输出形状: torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"输入文本:\", batch)\n",
    "print(\"输出形状:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d616e7a-568b-4921-af29-bd3f4683cd2e",
   "metadata": {},
   "source": [
    "- 我们将在下一章训练这个模型\n",
    "- 但是，关于其大小的一个快速说明：我们之前将其称为124M参数模型；我们可以如下双重检查这个数字："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "84fb8be4-9d3b-402b-b3da-86b663aac33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数总数: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"参数总数: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d13dd-dd01-4ba6-a2ad-31ca8a9fd660",
   "metadata": {},
   "source": [
    "- 如我们上面看到的，这个模型有163M而不是124M参数；为什么？\n",
    "- 在原始GPT-2论文中，研究人员应用了权重绑定，这意味着他们重用了token嵌入层（`tok_emb`）作为输出层，这意味着设置 `self.out_head.weight = self.tok_emb.weight`\n",
    "- token嵌入层将50,257维的独热编码输入token投影到768维的嵌入表示\n",
    "- 输出层将768维嵌入投影回50,257维表示，以便我们可以将这些转换回单词（下一节将详细介绍）\n",
    "- 因此，嵌入层和输出层具有相同数量的权重参数，正如我们基于其权重矩阵的形状所看到的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e3b43233-e9b8-4f5a-b72b-a263ec686982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token嵌入层形状: torch.Size([50257, 768])\n",
      "输出层形状: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token嵌入层形状:\", model.tok_emb.weight.shape)\n",
    "print(\"输出层形状:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02259f6-6f79-4c89-a866-4ebeae1c3289",
   "metadata": {},
   "source": [
    "- 在原始GPT-2论文中，研究人员重用了token嵌入矩阵作为输出矩阵\n",
    "- 相应地，如果我们减去输出层的参数数量，我们就会得到一个124M参数模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e3b43233-e9b8-4f5a-b72b-a263ec686982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "减去输出层后的参数数量: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"减去输出层后的参数数量: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02259f6-6f79-4c89-a866-4ebeae1c3289",
   "metadata": {},
   "source": [
    "- 现在我们得到了124M参数，这与原始GPT-2模型的大小相匹配\n",
    "- 在本书中，我们将保持嵌入层和输出层分离，以保持代码简单和清晰\n",
    "- 但是，如果您想要实现权重绑定以减少内存需求，您可以在模型初始化后添加以下行：\n",
    "\n",
    "```python\n",
    "model.out_head.weight = model.tok_emb.weight\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b03f80-b94c-46e7-9d42-d0df399ff3db",
   "metadata": {},
   "source": [
    "- 在实践中，我发现不使用权重绑定更容易训练模型，这就是为什么我们在这里没有实现它\n",
    "- 但是，当我们在第5章加载预训练权重时，我们将重新审视并应用这种权重绑定的想法\n",
    "- 最后，我们可以如下计算模型的内存需求，这可以是一个有用的参考点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5131a752-fab8-4d70-a600-e29870b33528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型总大小: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# 计算总大小（以字节为单位）（假设float32，每个参数4字节）\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# 转换为兆字节\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"模型总大小: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a3be4-c20a-4657-b4e0-77c97510b47c",
   "metadata": {},
   "source": [
    "- 练习：您也可以尝试以下其他配置，这些配置在[GPT-2论文](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=dOad5HoAAAAJ&citation_for_view=dOad5HoAAAAJ:YsMSGLbcyi4C)中有所引用。\n",
    "\n",
    "    - **GPT2-small**（我们已经实现的124M配置）：\n",
    "        - \"emb_dim\" = 768\n",
    "        - \"n_layers\" = 12\n",
    "        - \"n_heads\" = 12\n",
    "\n",
    "    - **GPT2-medium:**\n",
    "        - \"emb_dim\" = 1024\n",
    "        - \"n_layers\" = 24\n",
    "        - \"n_heads\" = 16\n",
    "    \n",
    "    - **GPT2-large:**\n",
    "        - \"emb_dim\" = 1280\n",
    "        - \"n_layers\" = 36\n",
    "        - \"n_heads\" = 20\n",
    "    \n",
    "    - **GPT2-XL:**\n",
    "        - \"emb_dim\" = 1600\n",
    "        - \"n_layers\" = 48\n",
    "        - \"n_heads\" = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d9bc0-95ab-45d4-9378-417628d86e35",
   "metadata": {},
   "source": [
    "## 4.7 生成文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da5deb-6ee0-4b9b-8dd2-abed7ed65172",
   "metadata": {},
   "source": [
    "- 像我们上面实现的GPT模型这样的LLM用于一次生成一个单词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caade12a-fe97-480f-939c-87d24044edff",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/16.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7061524-a3bd-4803-ade6-2e3b7b79ac13",
   "metadata": {},
   "source": [
    "- 以下 `generate_text_simple` 函数实现了贪婪解码，这是一种简单快速的文本生成方法\n",
    "- 在贪婪解码中，在每一步，模型选择概率最高的单词（或token）作为其下一个输出（最高的logit对应最高的概率，所以我们技术上甚至不必显式计算softmax函数）\n",
    "- 在下一章中，我们将实现一个更高级的 `generate_text` 函数\n",
    "- 下图描述了GPT模型如何在给定输入上下文的情况下生成下一个单词token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0f32c-c18c-445e-b294-a879de2aa187",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/17.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c9b428a9-8764-4b36-80cd-7d4e00595ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx是当前上下文中索引的(batch, n_tokens)数组\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # 如果当前上下文超过支持的上下文大小，则裁剪\n",
    "        # 例如，如果LLM只支持5个token，而上下文大小是10\n",
    "        # 那么只有最后5个token被用作上下文\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # 获取预测\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # 只关注最后一个时间步\n",
    "        # (batch, n_tokens, vocab_size) 变成 (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # 应用softmax获取概率\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # 获取概率值最高的词汇条目的索引\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # 将采样的索引附加到运行序列\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515f2c1-3cc7-421c-8d58-cc2f563b7030",
   "metadata": {},
   "source": [
    "- 上面的 `generate_text_simple` 实现了一个迭代过程，它一次创建一个token\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/18.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682eac4-f9bd-438b-9dec-6b1cc7bc05ce",
   "metadata": {},
   "source": [
    "- 让我们准备一个输入示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3d7e3e94-df0f-4c0f-a6a1-423f500ac1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码: [15496, 11, 314, 716]\n",
      "编码张量形状: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"编码:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"编码张量形状:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a72a9b60-de66-44cf-b2f9-1e638934ada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "输出长度: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # 禁用dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"输出:\", out)\n",
    "print(\"输出长度:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d131c00-1787-44ba-bec3-7c145497b2c3",
   "metadata": {},
   "source": [
    "- 移除批次维度并转换回文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "053d99f6-5710-4446-8d52-117fb34ea9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a894003-51f6-4ccc-996f-3b9c7d5a1d70",
   "metadata": {},
   "source": [
    "- 注意模型是未训练的；因此上面的输出文本是随机的\n",
    "- 我们将在下一章训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35278b6-9e5c-480f-83e5-011a1173648f",
   "metadata": {},
   "source": [
    "## 总结和要点\n",
    "\n",
    "- 请参阅 [./gpt.py](./gpt.py) 脚本，这是一个包含我们在此Jupyter notebook中实现的GPT模型的独立脚本\n",
    "- 您可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 中找到练习解答"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-from-scratch (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
